{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8164e8a6-8ac9-42dd-bf38-0fb33f8c6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import transformers\n",
    "import optimum\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from trl import DPOTrainer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "#from time \n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94566def-f6eb-44aa-961f-3bd5c2942383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 51.041271808 GB, Free Memory: 51.041271808 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "free_memory = total_memory - torch.cuda.memory_allocated(0)\n",
    "print(f\"Total GPU Memory: {total_memory / 1e9} GB, Free Memory: {free_memory / 1e9} GB\")\n",
    "\n",
    "\n",
    "#import flash-attention\n",
    "\n",
    "model_path = \"google/gemma-2b-it\" # \"google/gemma-2b-it\" \"microsoft/phi-2\"\n",
    "access_token = \"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\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabb3b1d-b1cb-45e9-bc60-1786954d13cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e27e5e202f51418fa358909ca41e05e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    "    token=access_token\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, token=access_token) #microsoft/phi-2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "output_dir = \"/llm_recovery/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af59472-2eb5-4b6e-adee-780a8630e379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected', 'chosen_score', 'rejected_score'],\n",
      "        num_rows: 11874\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected', 'chosen_score', 'rejected_score'],\n",
      "        num_rows: 120\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_path = '/llm_recovery/data_generation/dpo_dataset_v2.csv'   #'/llm_recovery/data_generation/dpo_dataset_v1.json'\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "dataset = Dataset.from_pandas(dataset)\n",
    "train_test_split = dataset.train_test_split(test_size=0.01)\n",
    "dataset_dict = DatasetDict(train=train_test_split['train'], test=train_test_split['test'])\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f916cb4-6f32-49a5-a9af-a8a32109a2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen: Style this as a letter deferring a job offer.\n",
      "Output: The rewrite prompt was used to convert the text from a conversational format to a formal one.\n",
      "Time taken: 1.8361783027648926 seconds\n",
      "Cosine similarity: 0.2677631378173828\n",
      "Cleaned Cosine similarity: 0.2677631378173828 \n",
      "\n",
      "Chosen: Rewrite the story with the triplets as different species of dogs or horses\n",
      "Output: The rewrite prompt was not provided in the context, so I cannot determine what it was.\n",
      "Time taken: 0.9222748279571533 seconds\n",
      "Cosine similarity: 0.07701173424720764\n",
      "Cleaned Cosine similarity: 0.07701173424720764 \n",
      "\n",
      "Chosen: Rewrite the following text, emphasizing the significance of key points while maintaining a professional demeanor.\n",
      "Output: The rewrite prompt was not provided in the context, so I cannot determine the rewrite prompt from the context.\n",
      "Time taken: 1.0335588455200195 seconds\n",
      "Cosine similarity: 0.11729010939598083\n",
      "Cleaned Cosine similarity: 0.11729010939598083 \n",
      "\n",
      "Chosen: Convert this into a technical specification document for a product.\n",
      "Output: Sure, here is the rewrite prompt:\n",
      "\n",
      "**Create a technical specification for a profile of Lars Peter Fredén, a Swedish diplomat. Include the following information in the product:**\n",
      "\n",
      "* Name: Lars Peter Fredén\n",
      "* Nationality: Swedish\n",
      "* Career highlights:\n",
      "    * Ambassadorships held\n",
      "    * Other accomplishments\n",
      "Time taken: 3.0437941551208496 seconds\n",
      "Cosine similarity: 0.36162281036376953\n",
      "Cleaned Cosine similarity: 0.36162281036376953 \n",
      "\n",
      "Chosen: Revise the given essay from a more antagonistic perspective, emphasizing the father-child power dynamic and the speaker's growing independence, while incorporating metaphors of speed, control, and breaking free from outdated expectations.\n",
      "Output: The rewrite prompt was not provided in the context, so I cannot determine what it was.\n",
      "Time taken: 0.9307641983032227 seconds\n",
      "Cosine similarity: 0.029294155538082123\n",
      "Cleaned Cosine similarity: 0.029294155538082123 \n",
      "\n",
      "Average Cosine Similarity: 0.1705963894724846\n",
      "*** Next ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor result in results:\\n    #print(\"Prompt:\", result[\\'prompt\\'])\\n    print(\"Chosen:\", result[\\'chosen\\'])\\n    print(\"Output:\", result[\\'output\\'])\\n    print(\"Time taken:\", result[\\'time_taken\\'], \"seconds\")\\n    print(\"Cosine similarity:\", result[\\'cosine_similarity\\'], \"\\n\")\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load a sentence transformer model for embedding calculation\n",
    "sentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def evaluate_model_with_similarity(test_dataset, tokenizer, model, num_samples=10):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    #model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    def clean_output_text(output_text):\n",
    "        # Define a list of filler phrases to remove\n",
    "        fillers = [\n",
    "            \"Rewrite prompt:\",\n",
    "            \"Sure, the rewrite prompt was used to convert the following text:\",\n",
    "            \"The rewrite prompt used to convert the text is:\"\n",
    "        ]\n",
    "        \n",
    "        # Iterate through each filler and remove it from the output text\n",
    "        for filler in fillers:\n",
    "            output_text = output_text.replace(filler, \"\").strip()\n",
    "        \n",
    "        return output_text\n",
    "        \n",
    "    results = []\n",
    "    random_indices = random.sample(range(len(test_dataset)), num_samples)\n",
    "\n",
    "    for i in random_indices:\n",
    "        test_sample = test_dataset[i] \n",
    "        \n",
    "        # Assuming 'test_sample' contains 'chosen' which we compare with the output\n",
    "        chosen_text = test_sample['chosen']\n",
    "        \"\"\"\n",
    "        prompt = create_custom_prompt(\n",
    "            tokenizer,\n",
    "            test_sample['original_text'],\n",
    "            test_sample['rewritten_text']\n",
    "        )\n",
    "        \"\"\"\n",
    "        inputs = tokenizer.encode(test_sample[\"prompt\"], add_special_tokens=False, return_tensors=\"pt\")\n",
    "        input_length = inputs.shape[1]\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n",
    "        new_tokens = outputs[0, input_length:]\n",
    "        generated_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "        cleaned_gen_text = clean_output_text(generated_text)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "\n",
    "        # Compute embeddings for both chosen and generated text\n",
    "        chosen_embedding = sentence_model.encode(chosen_text, convert_to_tensor=True)\n",
    "        generated_embedding = sentence_model.encode(generated_text, convert_to_tensor=True)\n",
    "        cleaned_embedding = sentence_model.encode(cleaned_gen_text, convert_to_tensor=True)\n",
    "\n",
    "        # Compute Cosine similarity\n",
    "        cosine_similarity = util.cos_sim(chosen_embedding, generated_embedding).item()\n",
    "        cosine_similarity2 = util.cos_sim(chosen_embedding, cleaned_embedding).item()\n",
    "        \n",
    "        \n",
    "        #similarity_scores = cosine_similarity(prompt_embeddings, prompt_1_embeddings)\n",
    "        #similarity_scores = np.diag(similarity_scores)\n",
    "\n",
    "        results.append({\n",
    "            'prompt': test_sample[\"prompt\"],\n",
    "            'chosen': chosen_text,\n",
    "            'output': generated_text,\n",
    "            'time_taken': time_taken,\n",
    "            'cosine_similarity': cosine_similarity,\n",
    "            'cleaned_cos_sim': cosine_similarity2\n",
    "        })\n",
    "\n",
    "    for result in results:\n",
    "        #print(\"Prompt:\", result['prompt'])\n",
    "        print(\"Chosen:\", result['chosen'])\n",
    "        print(\"Output:\", result['output'])\n",
    "        print(\"Time taken:\", result['time_taken'], \"seconds\")\n",
    "        print(\"Cosine similarity:\", result['cosine_similarity'])\n",
    "        print(\"Cleaned Cosine similarity:\", result['cleaned_cos_sim'], \"\\n\")\n",
    "\n",
    "        # Calculate the average cosine similarity\n",
    "    total_similarity = sum(result['cosine_similarity'] for result in results)\n",
    "    average_similarity = total_similarity / len(results)\n",
    "    \n",
    "    # Print the average similarity\n",
    "    print(\"Average Cosine Similarity:\", average_similarity)\n",
    "\n",
    "    print(\"*** Next ***\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "results = evaluate_model_with_similarity(dataset_dict[\"train\"], tokenizer, base_model, num_samples=5)\n",
    "\n",
    "\"\"\"\n",
    "for result in results:\n",
    "    #print(\"Prompt:\", result['prompt'])\n",
    "    print(\"Chosen:\", result['chosen'])\n",
    "    print(\"Output:\", result['output'])\n",
    "    print(\"Time taken:\", result['time_taken'], \"seconds\")\n",
    "    print(\"Cosine similarity:\", result['cosine_similarity'], \"\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7633fc2-c740-4cf5-9cc8-c594603c1189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'chosen', 'rejected', 'chosen_score', 'rejected_score'],\n",
      "    num_rows: 4000\n",
      "}) Dataset({\n",
      "    features: ['prompt', 'chosen', 'rejected', 'chosen_score', 'rejected_score'],\n",
      "    num_rows: 120\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Assuming dataset[\"train\"] and dataset[\"test\"] are your original datasets\n",
    "train_subset = dataset_dict[\"train\"].shuffle(seed=42).select(range(4000))\n",
    "test_subset = dataset_dict[\"test\"].shuffle(seed=42)\n",
    "\n",
    "# You can now use train_subset and test_subset for training and evaluation\n",
    "print(train_subset, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d406de8f-aecf-4c04-81ed-fc1b5453877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "class PushToGitHubCallback(TrainerCallback):\n",
    "    def __init__(self, output_dir, commit_message=\"Update model\"):\n",
    "        self.output_dir = output_dir\n",
    "        self.commit_message = commit_message\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        print(\"Pushing model checkpoint to GitHub...\")\n",
    "        try:\n",
    "            # Ensure we're in the correct directory\n",
    "            os.chdir(self.output_dir)\n",
    "\n",
    "            # Add all files to Git\n",
    "            subprocess.run([\"git\", \"add\", \".\"], check=True)\n",
    "            \n",
    "            # Commit changes\n",
    "            subprocess.run([\"git\", \"commit\", \"-m\", self.commit_message], check=True)\n",
    "            \n",
    "            # Push changes\n",
    "            subprocess.run([\"git\", \"push\"], check=True)\n",
    "            \n",
    "            print(\"Model checkpoint successfully pushed to GitHub.\")\n",
    "            \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to push to GitHub: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54fcfdac-1c61-43cf-8ef9-824f60f5c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import time\n",
    "\n",
    "class EvaluateCallback(TrainerCallback):\n",
    "    def __init__(self, eval_function, eval_dataset, tokenizer, num_samples=10, eval_steps=50):\n",
    "        \"\"\"\n",
    "        eval_function: The evaluation function to use.\n",
    "        eval_dataset: The dataset to use for evaluation.\n",
    "        tokenizer: The tokenizer for encoding.\n",
    "        num_samples: Number of samples to evaluate.\n",
    "        eval_steps: Frequency of evaluation in terms of training steps.\n",
    "        \"\"\"\n",
    "        self.eval_function = eval_function\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_samples = num_samples\n",
    "        self.eval_steps = eval_steps\n",
    "        self.step_count = 0\n",
    "\n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.eval_steps == 0:\n",
    "            print(\"\\nRunning evaluation...\")\n",
    "            self.eval_function(\n",
    "                test_dataset=self.eval_dataset, \n",
    "                tokenizer=self.tokenizer, \n",
    "                model=model, \n",
    "                num_samples=self.num_samples\n",
    "            )\n",
    "\n",
    "# Instantiate the custom callback\n",
    "eval_callback = EvaluateCallback(\n",
    "    eval_function=evaluate_model_with_similarity,\n",
    "    eval_dataset=test_subset,  # Assuming this is a slice of your dataset\n",
    "    tokenizer=tokenizer,\n",
    "    num_samples=20,  # Adjust the number of samples for evaluation\n",
    "    eval_steps=50  # Evaluate every 50 steps, adjust as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3bc8c30-96b1-4f30-bcef-15a3b1e7c300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaFlashAttention2(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dde552-fc87-4de0-93ba-ca95fecb10ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,805,824 || all params: 2,515,978,240 || trainable%: 0.3897420034920493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:272: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:332: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bfd6ae9b87c4473b33b15d7c8b20d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='667' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 38/667 03:01 < 52:47, 0.20 it/s, Epoch 0.06/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# from https://github.com/mlabonne/llm-course/blob/main/Fine_tune_a_Mistral_7b_model_with_DPO.ipynb\n",
    "lora_dropout=0.1\n",
    "lora_r=8 #\n",
    "lora_alpha=32 #\n",
    "learning_rate=1e-6 # 5e-4 5e-5\n",
    "batch_size = 3\n",
    "dpo_beta = 0.1 # 0.1\n",
    "\n",
    "def create_peft_config(model):\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        lora_dropout=lora_dropout,\n",
    "        lora_alpha=lora_alpha,\n",
    "        r=lora_r,\n",
    "        bias=\"none\",\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return model, peft_config\n",
    "\n",
    "model, lora_config = create_peft_config(base_model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "\n",
    "    gradient_accumulation_steps=2,\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_steps=20,  #50\n",
    "    logging_steps=50,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=50,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model, # model base_model\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset, # test_dataset dataset dataset[\"train\"]\n",
    "    #test_dataset=dataset[\"test\"],\n",
    "    callbacks=[eval_callback, PushToGitHubCallback(output_dir=output_dir, commit_message=\"Update model checkpoint\")],  # Add the custom callback\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=lora_config,\n",
    "    beta=dpo_beta,\n",
    "    max_prompt_length=1024, #changed from 1024\n",
    "    max_length=1024, #1536\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Starting trainer...\")\n",
    "trainer.train()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1185c-6c42-4c9c-9837-429463ead464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# todo: during training getting these warning:\n",
    "# i guess this is on the base model, need to check. in that case this is fine\n",
    "# UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
    "\n",
    "# seems that this can be ignored:\n",
    "# Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
    "model_name = \"gemma_2b_it\"\n",
    "output_dir = os.path.join(output_dir, f\"final_checkpoint_{model_name}\")\n",
    "trainer.model.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d80fe1f-33d1-4e96-bdf7-137dc0b771fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def push_changes_to_github(output_dir, commit_message=\"Update model\"):\n",
    "    \"\"\"\n",
    "    Pushes changes in output_dir to the existing GitHub repository.\n",
    "    \n",
    "    Parameters:\n",
    "    - output_dir: Path to the directory containing changes to push.\n",
    "    - commit_message: Commit message for the changes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Add all files to Git\n",
    "        subprocess.run([\"git\", \"add\", \".\"], cwd=output_dir, check=True)\n",
    "        \n",
    "        # Commit changes\n",
    "        subprocess.run([\"git\", \"commit\", \"-m\", commit_message], cwd=output_dir, check=True)\n",
    "        \n",
    "        # Push changes\n",
    "        subprocess.run([\"git\", \"push\"], cwd=output_dir, check=True)\n",
    "        \n",
    "        print(\"Changes successfully pushed to GitHub.\")\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "commit_message = \"Update model with new training data\"  # Customize your commit message\n",
    "push_changes_to_github(output_dir, commit_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70be69-31a9-4f75-ac26-54a8b3697edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
