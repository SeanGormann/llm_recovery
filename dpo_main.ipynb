{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8164e8a6-8ac9-42dd-bf38-0fb33f8c6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import transformers\n",
    "import optimum\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from trl import DPOTrainer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "#from time \n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94566def-f6eb-44aa-961f-3bd5c2942383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 25.435766784 GB, Free Memory: 25.435766784 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "free_memory = total_memory - torch.cuda.memory_allocated(0)\n",
    "print(f\"Total GPU Memory: {total_memory / 1e9} GB, Free Memory: {free_memory / 1e9} GB\")\n",
    "\n",
    "\n",
    "#import flash-attention\n",
    "\n",
    "model_path = \"microsoft/phi-2\" # \"google/gemma-2b-it\" \"microsoft/phi-2\" \"stabilityai/stablelm-zephyr-3b\" \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "access_token = \"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\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabb3b1d-b1cb-45e9-bc60-1786954d13cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fc5cb6f88549acaeb00f10d520bebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    "    token=access_token\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, token=access_token) #microsoft/phi-2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "output_dir = \"/llm_recovery/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f494a73-0c23-4240-83fc-8b3ece395e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3476\n",
      "3476\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "      <th>chosen_score</th>\n",
       "      <th>rejected_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Instruction: Analyze the differences between t...</td>\n",
       "      <td>[rp] Embark on an extended journey of interpre...</td>\n",
       "      <td>[rp] Analyze this text and evaluate its streng...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Instruction: Analyze the differences between t...</td>\n",
       "      <td>[rp] Embark on an extended journey of interpre...</td>\n",
       "      <td>[rp] Analyze and provide feedback on the stren...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Instruction: Analyze the differences between t...</td>\n",
       "      <td>[rp] Embark on an extended journey of interpre...</td>\n",
       "      <td>[rp] Analyze this text for its strengths and w...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Instruction: Analyze the differences between t...</td>\n",
       "      <td>[rp] Embark on an extended journey of interpre...</td>\n",
       "      <td>[rp] Analyze these two texts and provide an an...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Instruction: Analyze the differences between t...</td>\n",
       "      <td>[rp] Embark on an extended journey of interpre...</td>\n",
       "      <td>[rp] Analyze and improve this text, highlighti...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Instruction: Analyze the differences between t...   \n",
       "1  Instruction: Analyze the differences between t...   \n",
       "2  Instruction: Analyze the differences between t...   \n",
       "3  Instruction: Analyze the differences between t...   \n",
       "4  Instruction: Analyze the differences between t...   \n",
       "\n",
       "                                              chosen  \\\n",
       "0  [rp] Embark on an extended journey of interpre...   \n",
       "1  [rp] Embark on an extended journey of interpre...   \n",
       "2  [rp] Embark on an extended journey of interpre...   \n",
       "3  [rp] Embark on an extended journey of interpre...   \n",
       "4  [rp] Embark on an extended journey of interpre...   \n",
       "\n",
       "                                            rejected  chosen_score  \\\n",
       "0  [rp] Analyze this text and evaluate its streng...             5   \n",
       "1  [rp] Analyze and provide feedback on the stren...             5   \n",
       "2  [rp] Analyze this text for its strengths and w...             5   \n",
       "3  [rp] Analyze these two texts and provide an an...             5   \n",
       "4  [rp] Analyze and improve this text, highlighti...             5   \n",
       "\n",
       "   rejected_score  \n",
       "0               3  \n",
       "1               2  \n",
       "2               3  \n",
       "3               3  \n",
       "4               1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = '/llm_recovery/data_generation/dpo_df_phi_v5.csv'   #'/llm_recovery/data_generation/dpo_dataset_v1.json'\n",
    "dataset = pd.read_csv(dataset_path, index_col=0)\n",
    "print(len(dataset))\n",
    "# Drop rows with NaN values\n",
    "dataset = dataset.dropna()\n",
    "print(len(dataset))\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da0e3490-892c-406d-ae9a-fb90da2232d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the end-of-sequence token to each value in the 'chosen' column\n",
    "dataset['chosen'] = dataset['chosen'].apply(lambda x: x.strip() + ' ' + tokenizer.eos_token)\n",
    "dataset['rejected'] = dataset['rejected'].apply(lambda x: x.strip() + ' ' + tokenizer.eos_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21221647-0337-49d4-8f6d-7b5c605727e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "      <th>chosen_score</th>\n",
       "      <th>rejected_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Instruction: Analyze the differences between t...</td>\n",
       "      <td>[rp] Embark on an extended journey of interpre...</td>\n",
       "      <td>[rp] Analyze this text and evaluate its streng...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Instruction: Analyze the differences between t...</td>\n",
       "      <td>[rp] Embark on an extended journey of interpre...</td>\n",
       "      <td>[rp] Analyze and provide feedback on the stren...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Instruction: Analyze the differences between t...</td>\n",
       "      <td>[rp] Embark on an extended journey of interpre...</td>\n",
       "      <td>[rp] Analyze this text for its strengths and w...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Instruction: Analyze the differences between t...</td>\n",
       "      <td>[rp] Embark on an extended journey of interpre...</td>\n",
       "      <td>[rp] Analyze these two texts and provide an an...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Instruction: Analyze the differences between t...</td>\n",
       "      <td>[rp] Embark on an extended journey of interpre...</td>\n",
       "      <td>[rp] Analyze and improve this text, highlighti...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3471</th>\n",
       "      <td>Instruction: Analyze the differences between t...</td>\n",
       "      <td>[rp] Elevate the descriptive elements in the f...</td>\n",
       "      <td>Rewrite from the text is used to convert the t...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3472</th>\n",
       "      <td>Instruction: Analyze the differences between t...</td>\n",
       "      <td>[rp] Edit this story to make it more suspensef...</td>\n",
       "      <td>Sure, here is the rewrite prompt: Provide the ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3473</th>\n",
       "      <td>Instruction: Analyze the differences between t...</td>\n",
       "      <td>[rp] Convert this JavaScript code to Java: [/r...</td>\n",
       "      <td>Sure, here is the rewrite prompt: **Rewrite pr...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3474</th>\n",
       "      <td>Instruction: Analyze the differences between t...</td>\n",
       "      <td>[rp] Compare and contrast the viewpoints prese...</td>\n",
       "      <td>Sure, here is the rewrite prompt: Provide the ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3475</th>\n",
       "      <td>Instruction: Analyze the differences between t...</td>\n",
       "      <td>[rp] Shorten this code while maintaining funct...</td>\n",
       "      <td>Sure, the rewrite prompt was not provided in t...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3476 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 prompt  \\\n",
       "0     Instruction: Analyze the differences between t...   \n",
       "1     Instruction: Analyze the differences between t...   \n",
       "2     Instruction: Analyze the differences between t...   \n",
       "3     Instruction: Analyze the differences between t...   \n",
       "4     Instruction: Analyze the differences between t...   \n",
       "...                                                 ...   \n",
       "3471  Instruction: Analyze the differences between t...   \n",
       "3472  Instruction: Analyze the differences between t...   \n",
       "3473  Instruction: Analyze the differences between t...   \n",
       "3474  Instruction: Analyze the differences between t...   \n",
       "3475  Instruction: Analyze the differences between t...   \n",
       "\n",
       "                                                 chosen  \\\n",
       "0     [rp] Embark on an extended journey of interpre...   \n",
       "1     [rp] Embark on an extended journey of interpre...   \n",
       "2     [rp] Embark on an extended journey of interpre...   \n",
       "3     [rp] Embark on an extended journey of interpre...   \n",
       "4     [rp] Embark on an extended journey of interpre...   \n",
       "...                                                 ...   \n",
       "3471  [rp] Elevate the descriptive elements in the f...   \n",
       "3472  [rp] Edit this story to make it more suspensef...   \n",
       "3473  [rp] Convert this JavaScript code to Java: [/r...   \n",
       "3474  [rp] Compare and contrast the viewpoints prese...   \n",
       "3475  [rp] Shorten this code while maintaining funct...   \n",
       "\n",
       "                                               rejected  chosen_score  \\\n",
       "0     [rp] Analyze this text and evaluate its streng...             5   \n",
       "1     [rp] Analyze and provide feedback on the stren...             5   \n",
       "2     [rp] Analyze this text for its strengths and w...             5   \n",
       "3     [rp] Analyze these two texts and provide an an...             5   \n",
       "4     [rp] Analyze and improve this text, highlighti...             5   \n",
       "...                                                 ...           ...   \n",
       "3471  Rewrite from the text is used to convert the t...             5   \n",
       "3472  Sure, here is the rewrite prompt: Provide the ...             5   \n",
       "3473  Sure, here is the rewrite prompt: **Rewrite pr...             5   \n",
       "3474  Sure, here is the rewrite prompt: Provide the ...             5   \n",
       "3475  Sure, the rewrite prompt was not provided in t...             5   \n",
       "\n",
       "      rejected_score  \n",
       "0                  3  \n",
       "1                  2  \n",
       "2                  3  \n",
       "3                  3  \n",
       "4                  1  \n",
       "...              ...  \n",
       "3471               0  \n",
       "3472               0  \n",
       "3473               0  \n",
       "3474               0  \n",
       "3475               0  \n",
       "\n",
       "[3476 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace <rp> and </rp> with [rp] and [/rp] in both 'chosen' and 'rejected' columns\n",
    "dataset['prompt'] = dataset['prompt'].str.replace(\"<rp>\", \"[rp]\").str.replace(\"</rp>\", \"[/rp]\")\n",
    "dataset['chosen'] = dataset['chosen'].str.replace(\"<rp>\", \"[rp]\").str.replace(\"</rp>\", \"[/rp]\")\n",
    "dataset['rejected'] = dataset['rejected'].str.replace(\"<rp>\", \"[rp]\").str.replace(\"</rp>\", \"[/rp]\")\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a469a80-b692-421b-bb9b-ea0b32433a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected', 'chosen_score', 'rejected_score', '__index_level_0__'],\n",
      "        num_rows: 3302\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected', 'chosen_score', 'rejected_score', '__index_level_0__'],\n",
      "        num_rows: 174\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(dataset)\n",
    "train_test_split = dataset.train_test_split(test_size=0.05)\n",
    "dataset_dict = DatasetDict(train=train_test_split['train'], test=train_test_split['test'])\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f916cb4-6f32-49a5-a9af-a8a32109a2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen: [rp] Optimize this code for better performance: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original code is a C# class called `MemoryBlock` that allocates and deallocates a block of memory. While the code is functional, there are some areas where it could be optimized for better performance: [rp]\n",
      "\n",
      "**1. Memory Allocation:**\n",
      "\n",
      "- The current code uses `Marshal.AllocHGlobal` to allocate the memory, which is a safe but relatively slow function. A faster alternative is `Marshal.AllocHGlobalArray` which allocates memory for an array of a specific size. This can significantly reduce allocation overhead.\n",
      "\n",
      "**2. Memory Alignment:**\n",
      "\n",
      "- The code allocates memory in a single block, but the\n",
      "Time taken: 7.28993558883667 seconds\n",
      "Cosine similarity: 0.4437544643878937\n",
      "Cleaned Cosine similarity: 0.4437544643878937 \n",
      "\n",
      "Chosen: [rp] Optimize this code for better performance: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten using the [rp] Rewrite Prompt [/rp]\n",
      "\n",
      "Time taken: 1.1466093063354492 seconds\n",
      "Cosine similarity: 0.5139988660812378\n",
      "Cleaned Cosine similarity: 0.5139988660812378 \n",
      "\n",
      "Chosen: [rp] Transform this complaint into a compliment. [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was transformed into a compliment by highlighting the positive aspects of industrial agriculture and emphasizing the importance of understanding and managing its environmental impact. [/rp]\n",
      "\n",
      "Time taken: 1.7188425064086914 seconds\n",
      "Cosine similarity: 0.378406286239624\n",
      "Cleaned Cosine similarity: 0.378406286239624 \n",
      "\n",
      "Chosen: [rp] Infuse the following passage with personality and charm by incorporating emojis and expressive formatting to convey emotion and tone: [/rp] <|endoftext|>\n",
      "Output:  [rp] The rewritten text uses a conversational and engaging tone, incorporating humor and personal anecdotes to make the topic more relatable and interesting. It also addresses the moral and legal complexities of automation, highlighting the potential benefits and drawbacks, and emphasizing the need for thoughtful consideration and creative solutions. [/rp]\n",
      "\n",
      "Time taken: 2.863068103790283 seconds\n",
      "Cosine similarity: 0.34909123182296753\n",
      "Cleaned Cosine similarity: 0.34909123182296753 \n",
      "\n",
      "Chosen: [rp] Convert this into a rock song [/rp] <|endoftext|>\n",
      "Output:  [rp] The prompt used to convert the original text to the rewritten text was to create a song with a focus on the moral and legal complexities surrounding interpersonal relationships. [/rp]\n",
      "\n",
      "Time taken: 1.8032541275024414 seconds\n",
      "Cosine similarity: 0.6189604997634888\n",
      "Cleaned Cosine similarity: 0.6189604997634888 \n",
      "\n",
      "Average Cosine Similarity: 0.46084226965904235\n",
      "\n",
      "\n",
      "*** Next ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor result in results:\\n    #print(\"Prompt:\", result[\\'prompt\\'])\\n    print(\"Chosen:\", result[\\'chosen\\'])\\n    print(\"Output:\", result[\\'output\\'])\\n    print(\"Time taken:\", result[\\'time_taken\\'], \"seconds\")\\n    print(\"Cosine similarity:\", result[\\'cosine_similarity\\'], \"\\n\")\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load a sentence transformer model for embedding calculation\n",
    "sentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def evaluate_model_with_similarity(test_dataset, tokenizer, model, num_samples=10):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    #model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    def clean_output_text(output_text):\n",
    "        # Define a list of filler phrases to remove\n",
    "        fillers = [\n",
    "            \"Rewrite prompt:\",\n",
    "            \"Sure, the rewrite prompt was used to convert the following text:\",\n",
    "            \"The rewrite prompt used to convert the text is:\"\n",
    "        ]\n",
    "        \n",
    "        # Iterate through each filler and remove it from the output text\n",
    "        for filler in fillers:\n",
    "            output_text = output_text.replace(filler, \"\").strip()\n",
    "        \n",
    "        return output_text\n",
    "        \n",
    "    results = []\n",
    "    random_indices = random.sample(range(len(test_dataset)), num_samples)\n",
    "\n",
    "    for i in random_indices:\n",
    "        test_sample = test_dataset[i] \n",
    "        \n",
    "        # Assuming 'test_sample' contains 'chosen' which we compare with the output\n",
    "        chosen_text = test_sample['chosen']\n",
    "        \"\"\"\n",
    "        prompt = create_custom_prompt(\n",
    "            tokenizer,\n",
    "            test_sample['original_text'],\n",
    "            test_sample['rewritten_text']\n",
    "        )\n",
    "        \"\"\"\n",
    "        inputs = tokenizer.encode(test_sample[\"prompt\"], add_special_tokens=False, return_tensors=\"pt\")\n",
    "        input_length = inputs.shape[1]\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n",
    "        new_tokens = outputs[0, input_length:]\n",
    "        generated_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "        cleaned_gen_text = clean_output_text(generated_text)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "\n",
    "        # Compute embeddings for both chosen and generated text\n",
    "        chosen_embedding = sentence_model.encode(chosen_text, convert_to_tensor=True)\n",
    "        generated_embedding = sentence_model.encode(generated_text, convert_to_tensor=True)\n",
    "        cleaned_embedding = sentence_model.encode(cleaned_gen_text, convert_to_tensor=True)\n",
    "\n",
    "        # Compute Cosine similarity\n",
    "        cosine_similarity = util.cos_sim(chosen_embedding, generated_embedding).item()\n",
    "        cosine_similarity2 = util.cos_sim(chosen_embedding, cleaned_embedding).item()\n",
    "        \n",
    "        \n",
    "        #similarity_scores = cosine_similarity(prompt_embeddings, prompt_1_embeddings)\n",
    "        #similarity_scores = np.diag(similarity_scores)\n",
    "\n",
    "        results.append({\n",
    "            'prompt': test_sample[\"prompt\"],\n",
    "            'chosen': chosen_text,\n",
    "            'output': generated_text,\n",
    "            'time_taken': time_taken,\n",
    "            'cosine_similarity': cosine_similarity,\n",
    "            'cleaned_cos_sim': cosine_similarity2\n",
    "        })\n",
    "\n",
    "    for result in results:\n",
    "        #print(\"Prompt:\", result['prompt'])\n",
    "        print(\"Chosen:\", result['chosen'])\n",
    "        print(\"Output:\", result['output'])\n",
    "        print(\"Time taken:\", result['time_taken'], \"seconds\")\n",
    "        print(\"Cosine similarity:\", result['cosine_similarity'])\n",
    "        print(\"Cleaned Cosine similarity:\", result['cleaned_cos_sim'], \"\\n\")\n",
    "\n",
    "        # Calculate the average cosine similarity\n",
    "    total_similarity = sum(result['cosine_similarity'] for result in results)\n",
    "    average_similarity = total_similarity / len(results)\n",
    "    \n",
    "    # Print the average similarity\n",
    "    print(\"Average Cosine Similarity:\", average_similarity)\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Next ***\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "results = evaluate_model_with_similarity(dataset_dict[\"train\"], tokenizer, base_model, num_samples=5)\n",
    "\n",
    "\"\"\"\n",
    "for result in results:\n",
    "    #print(\"Prompt:\", result['prompt'])\n",
    "    print(\"Chosen:\", result['chosen'])\n",
    "    print(\"Output:\", result['output'])\n",
    "    print(\"Time taken:\", result['time_taken'], \"seconds\")\n",
    "    print(\"Cosine similarity:\", result['cosine_similarity'], \"\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7633fc2-c740-4cf5-9cc8-c594603c1189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'chosen', 'rejected', 'chosen_score', 'rejected_score', '__index_level_0__'],\n",
      "    num_rows: 3302\n",
      "}) Dataset({\n",
      "    features: ['prompt', 'chosen', 'rejected', 'chosen_score', 'rejected_score', '__index_level_0__'],\n",
      "    num_rows: 174\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Assuming dataset[\"train\"] and dataset[\"test\"] are your original datasets\n",
    "train_subset = dataset_dict[\"train\"].shuffle(seed=42) #.select(range(8000))\n",
    "test_subset = dataset_dict[\"test\"].shuffle(seed=42)\n",
    "\n",
    "# You can now use train_subset and test_subset for training and evaluation\n",
    "print(train_subset, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d406de8f-aecf-4c04-81ed-fc1b5453877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "class PushToGitHubCallback(TrainerCallback):\n",
    "    def __init__(self, output_dir, commit_message=\"Update model\"):\n",
    "        self.output_dir = output_dir\n",
    "        self.commit_message = commit_message\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        print(\"Pushing model checkpoint to GitHub...\")\n",
    "        try:\n",
    "            # Ensure we're in the correct directory\n",
    "            os.chdir(self.output_dir)\n",
    "\n",
    "            # Add all files to Git\n",
    "            subprocess.run([\"git\", \"add\", \".\"], check=True)\n",
    "            \n",
    "            # Commit changes\n",
    "            subprocess.run([\"git\", \"commit\", \"-m\", self.commit_message], check=True)\n",
    "            \n",
    "            # Push changes\n",
    "            subprocess.run([\"git\", \"push\"], check=True)\n",
    "            \n",
    "            print(\"Model checkpoint successfully pushed to GitHub.\")\n",
    "            \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to push to GitHub: {e}\")\n",
    "\n",
    "\n",
    "class MetricsLoggingCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback for logging additional metrics during training.\"\"\"\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # This method gets called every logging step.\n",
    "        # You can access the log history using `state.log_history` which is a list of dictionaries.\n",
    "        # The most recent log entry is at the end of this list.\n",
    "        if state.log_history:\n",
    "            latest_log = state.log_history[-1]\n",
    "            # Extract the metrics you're interested in. Here's an example:\n",
    "            metrics_of_interest = [\"rewards/chosen\", \"rewards/rejected\", \"rewards/accuracies\", \"rewards/margins\", \"logps/rejected\", \"logps/chosen\"]\n",
    "            metrics_str = \", \".join(f\"{metric}: {latest_log.get(metric, 'n/a')}\" for metric in metrics_of_interest)\n",
    "            print(f\"Step: {state.global_step}, {metrics_str}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54fcfdac-1c61-43cf-8ef9-824f60f5c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import time\n",
    "\n",
    "class EvaluateCallback(TrainerCallback):\n",
    "    def __init__(self, eval_function, eval_dataset, tokenizer, num_samples=10, eval_steps=50):\n",
    "        \"\"\"\n",
    "        eval_function: The evaluation function to use.\n",
    "        eval_dataset: The dataset to use for evaluation.\n",
    "        tokenizer: The tokenizer for encoding.\n",
    "        num_samples: Number of samples to evaluate.\n",
    "        eval_steps: Frequency of evaluation in terms of training steps.\n",
    "        \"\"\"\n",
    "        self.eval_function = eval_function\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_samples = num_samples\n",
    "        self.eval_steps = eval_steps\n",
    "        self.step_count = 0\n",
    "\n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.eval_steps == 0:\n",
    "            print(\"\\nRunning evaluation...\")\n",
    "            self.eval_function(\n",
    "                test_dataset=self.eval_dataset, \n",
    "                tokenizer=self.tokenizer, \n",
    "                model=model, \n",
    "                num_samples=self.num_samples\n",
    "            )\n",
    "\n",
    "# Instantiate the custom callback\n",
    "eval_callback = EvaluateCallback(\n",
    "    eval_function=evaluate_model_with_similarity,\n",
    "    eval_dataset=test_subset,  # Assuming this is a slice of your dataset\n",
    "    tokenizer=tokenizer,\n",
    "    num_samples=5,  # Adjust the number of samples for evaluation\n",
    "    eval_steps=10  # Evaluate every 50 steps, adjust as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3bc8c30-96b1-4f30-bcef-15a3b1e7c300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForCausalLM(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2560)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhiDecoderLayer(\n",
       "        (self_attn): PhiFlashAttention2(\n",
       "          (q_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (dense): Linear4bit(in_features=2560, out_features=2560, bias=True)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear4bit(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear4bit(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=51200, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5dde552-fc87-4de0-93ba-ca95fecb10ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 48,906,240 || all params: 2,828,590,080 || trainable%: 1.728997083946501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:272: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:332: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916ff47e0c6d4009993bb09fa4ecd0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3302 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4528 > 2048). Running this sequence through the model will result in indexing errors\n",
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Using auto half precision backend\n",
      "Currently training with a batch size of: 4\n",
      "***** Running training *****\n",
      "  Num examples = 3,302\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 8\n",
      "  Total optimization steps = 103\n",
      "  Number of trainable parameters = 48,906,240\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='103' max='103' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [103/103 1:14:42, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.693200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.691900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.690700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.690200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.687700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.686100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.688900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.684800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.687700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.686600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.684400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.683400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.683800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.681600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.686000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5, rewards/chosen: -0.001997766550630331, rewards/rejected: -0.002084240783005953, rewards/accuracies: 0.11249999701976776, rewards/margins: 8.647427603136748e-05, logps/rejected: -78.52970886230469, logps/chosen: -41.526893615722656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen: [rp] Make this into a haiku [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten using the haiku format. [/rp]\n",
      "\n",
      "Time taken: 1.8122167587280273 seconds\n",
      "Cosine similarity: 0.8608782291412354\n",
      "Cleaned Cosine similarity: 0.8608782291412354 \n",
      "\n",
      "Chosen: [rp] Shorten this code while maintaining functionality: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original code was rewritten to be more concise and efficient. [/rp]\n",
      "\n",
      "Time taken: 1.9924633502960205 seconds\n",
      "Cosine similarity: 0.6104882955551147\n",
      "Cleaned Cosine similarity: 0.6104882955551147 \n",
      "\n",
      "Chosen: [rp] Provide a more comprehensive analysis of the themes discussed in the following excerpt, offering deeper insights and explanations: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was converted to the rewritten text using the prompt: \"Rewrite the given passage to make it more suitable for a high school level examination. The rewritten text should be in the form of a passage that can be used for a high school level examination. The passage should be written in a formal tone and should include a minimum of 500 words. The passage should be written in a way that is suitable for a high school level examination. The passage should be written in a way that is suitable for a high school level examination. The passage should be written in a way that is suitable for a high school level examination. The passage should be written in a way that is suitable for a high school level examination. The passage should\n",
      "Time taken: 12.80519151687622 seconds\n",
      "Cosine similarity: 0.33489227294921875\n",
      "Cleaned Cosine similarity: 0.33489227294921875 \n",
      "\n",
      "Chosen: [rp] Shorten this code while maintaining functionality: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten using the Ruby code snippet provided. [/rp]\n",
      "\n",
      "Time taken: 1.9867095947265625 seconds\n",
      "Cosine similarity: 0.5427474975585938\n",
      "Cleaned Cosine similarity: 0.5427474975585938 \n",
      "\n",
      "Chosen: [rp] Optimize this code for better performance: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten to include more specific and detailed test cases for memory allocation in Java. [/rp]\n",
      "\n",
      "Time taken: 2.7160146236419678 seconds\n",
      "Cosine similarity: 0.5178021788597107\n",
      "Cleaned Cosine similarity: 0.5178021788597107 \n",
      "\n",
      "Average Cosine Similarity: 0.5733616948127747\n",
      "\n",
      "\n",
      "*** Next ***\n",
      "Step: 10, rewards/chosen: 0.006039338186383247, rewards/rejected: 0.0030554821714758873, rewards/accuracies: 0.3499999940395355, rewards/margins: 0.0029838562477380037, logps/rejected: -74.42095184326172, logps/chosen: -41.12699890136719\n",
      "Step: 15, rewards/chosen: 0.0034690476022660732, rewards/rejected: -0.001983074937015772, rewards/accuracies: 0.3812499940395355, rewards/margins: 0.005452122539281845, logps/rejected: -82.57380676269531, logps/chosen: -44.144161224365234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen: [rp] Improve this code by enhancing readability and applying best practices: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten to improve readability and adhere to best practices. [/rp]\n",
      "\n",
      "Time taken: 2.157893419265747 seconds\n",
      "Cosine similarity: 0.6205748915672302\n",
      "Cleaned Cosine similarity: 0.6205748915672302 \n",
      "\n",
      "Chosen: [rp] Broaden the scope of the following text by incorporating additional perspectives and insights: [/rp] <|endoftext|>\n",
      "Output:  [rp] <b>Rewrite Prompt:</b>\n",
      "\n",
      "Rewrite the original text into a high school level instruction following while keeping as many content as possible, using a lonely tone.\n",
      "\n",
      "<b>Rewritten Text:</b>\n",
      "\n",
      "In the course of a story, the dreams and aspirations of characters often evolve and change as they face new challenges and gain new experiences.\n",
      "\n",
      "In the beginning, a character's dreams and aspirations might be relatively simple and straightforward. For example, a young protagonist might dream of becoming a great warrior or a powerful sorcerer. As the story progresses, however, the character's dreams and aspirations might become more complex and nuanced. They might realize that their original goals were overly simplistic or\n",
      "Time taken: 12.944512844085693 seconds\n",
      "Cosine similarity: 0.4194600582122803\n",
      "Cleaned Cosine similarity: 0.4194600582122803 \n",
      "\n",
      "Chosen: [rp] Convert this into a blues song [/rp] <|endoftext|>\n",
      "Output:  [rp] The prompt used to convert the original text to the rewritten text was to analyze the differences between the original and the rewritten text in terms of tone, style, and content. [/rp]\n",
      "\n",
      "Time taken: 3.8217153549194336 seconds\n",
      "Cosine similarity: 0.5150067210197449\n",
      "Cleaned Cosine similarity: 0.5150067210197449 \n",
      "\n",
      "Chosen: [rp] Elevate the descriptive elements in the following text, engaging the reader's senses and imagination with rich, vivid descriptions: [/rp] <|endoftext|>\n",
      "Output:  [rp] The prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the\n",
      "Time taken: 12.679044246673584 seconds\n",
      "Cosine similarity: 0.3970605731010437\n",
      "Cleaned Cosine similarity: 0.3970605731010437 \n",
      "\n",
      "Chosen: [rp] Edit this story to make it more suspenseful. [/rp] <|endoftext|>\n",
      "Output:  [rp] The rewritten prompt used to convert the original text to the rewritten text is:\n",
      "\n",
      "[rp] The rewritten prompt used to convert the original text to the rewritten text is:\n",
      "\n",
      "[rp] The rewritten prompt used to convert the original text to the rewritten text is:\n",
      "\n",
      "[rp] The rewritten prompt used to convert the original text to the rewritten text is:\n",
      "\n",
      "[rp] The rewritten prompt used to convert the original text to the rewritten text is:\n",
      "\n",
      "[rp] The rewritten prompt used to convert the original text to the rewritten text is:\n",
      "\n",
      "[rp] The rewritten prompt used to convert the original text to the rewritten text is:\n",
      "\n",
      "[rp\n",
      "Time taken: 12.491440296173096 seconds\n",
      "Cosine similarity: 0.4678497910499573\n",
      "Cleaned Cosine similarity: 0.4678497910499573 \n",
      "\n",
      "Average Cosine Similarity: 0.48399040699005125\n",
      "\n",
      "\n",
      "*** Next ***\n",
      "Step: 20, rewards/chosen: 0.0015678355703130364, rewards/rejected: -0.004802193958312273, rewards/accuracies: 0.3187499940395355, rewards/margins: 0.0063700289465487, logps/rejected: -71.14688110351562, logps/chosen: -40.890037536621094\n",
      "Step: 25, rewards/chosen: 0.003552918555215001, rewards/rejected: -0.005558362230658531, rewards/accuracies: 0.4437499940395355, rewards/margins: 0.009111279621720314, logps/rejected: -80.27408599853516, logps/chosen: -43.553131103515625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen: [rp] Provide a more comprehensive analysis of the themes discussed in the following excerpt, offering deeper insights and explanations: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was converted to the rewritten text using the prompt: \"Rewrite the original text into a high school level instruction following while keeping as many content as possible, using a neutral tone.\" [/rp]\n",
      "\n",
      "Time taken: 4.291902780532837 seconds\n",
      "Cosine similarity: 0.4477202892303467\n",
      "Cleaned Cosine similarity: 0.4477202892303467 \n",
      "\n",
      "Chosen: [rp] Create a how-to guide based on the process described in this text. [/rp] <|endoftext|>\n",
      "Output:  [rp] The prompt used to convert the original text to the rewritten text is [rp] a step-by-step guide[/rp]\n",
      "\n",
      "Time taken: 2.8992557525634766 seconds\n",
      "Cosine similarity: 0.6188206076622009\n",
      "Cleaned Cosine similarity: 0.6188206076622009 \n",
      "\n",
      "Chosen: [rp] Draft an elevator pitch based on this research paper. [/rp] <|endoftext|>\n",
      "Output:  [rp] The cultural evolution of language and communication is a vast and complex topic, with roots stretching back to the earliest human prehistory. [rp] Early humans likely communicated using a combination of vocalizations, gestures, and body language. Over time, these rudimentary forms of communication evolved into more complex systems of spoken language, which allowed for the sharing of ideas, the development of culture, and the formation of larger social groups. [rp] The advent of written language some 5,000 years ago represented a major milestone in the cultural evolution of communication. Writing systems allowed for the recording of language and the preservation of knowledge across time and space, enabling the accumulation of cultural knowledge and the development of complex societies. [rp] With\n",
      "Time taken: 12.69667363166809 seconds\n",
      "Cosine similarity: 0.22388309240341187\n",
      "Cleaned Cosine similarity: 0.22388309240341187 \n",
      "\n",
      "Chosen: [rp] Convert this C# code to HTML: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was converted to the rewritten text using a prompt that included a list of things to look for when reviewing code, as well as links to resources for writing recursive algorithms in C#. [/rp]\n",
      "\n",
      "Time taken: 4.158716678619385 seconds\n",
      "Cosine similarity: 0.4925966262817383\n",
      "Cleaned Cosine similarity: 0.4925966262817383 \n",
      "\n",
      "Chosen: [rp] Improve this code by enhancing readability and applying best practices: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten using [rp] a more structured and organized approach, [rp] with improved readability and adherence to best practices. [rp] The rewritten code is now easier to understand and maintain. [rp] The original tokens: 474, [rp] the rewritten tokens: 437. [/rp]\n",
      "\n",
      "Time taken: 6.726475715637207 seconds\n",
      "Cosine similarity: 0.5677786469459534\n",
      "Cleaned Cosine similarity: 0.5677786469459534 \n",
      "\n",
      "Average Cosine Similarity: 0.47015985250473025\n",
      "\n",
      "\n",
      "*** Next ***\n",
      "Step: 30, rewards/chosen: 0.002762212883681059, rewards/rejected: -0.006622152868658304, rewards/accuracies: 0.4375, rewards/margins: 0.009384365752339363, logps/rejected: -83.96746063232422, logps/chosen: -49.39048767089844\n",
      "Step: 35, rewards/chosen: 0.005419764202088118, rewards/rejected: -0.006060753017663956, rewards/accuracies: 0.4375, rewards/margins: 0.01148051768541336, logps/rejected: -81.66938018798828, logps/chosen: -42.629520416259766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen: [rp] Highlight the divergent conclusions drawn by these studies. [/rp] <|endoftext|>\n",
      "Output:  [rp] The text highlights several challenges facing the importance of sports in society. These challenges include the commercialization of sports, the decline in youth participation, the need for greater diversity, equity, and inclusion, and the challenge of maintaining the integrity of sports. However, the text does not draw any divergent conclusions. It primarily focuses on outlining the challenges facing the importance of sports in society and emphasizes the need to address them to ensure sports continue to play a positive role. Therefore, the text does not highlight any divergent conclusions drawn by the studies. [rp]\n",
      "\n",
      "Time taken: 10.211620092391968 seconds\n",
      "Cosine similarity: 0.11835160851478577\n",
      "Cleaned Cosine similarity: 0.11835160851478577 \n",
      "\n",
      "Chosen: [rp] Make this into a free verse poem [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten using the prompt \"AI and Ethics\" [rp]\n",
      "\n",
      "Time taken: 2.0296738147735596 seconds\n",
      "Cosine similarity: 0.47709423303604126\n",
      "Cleaned Cosine similarity: 0.47709423303604126 \n",
      "\n",
      "Chosen: [rp] Summarize the main points of this article in one sentence. [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten to summarize the main points of the article in one sentence. [/rp]\n",
      "\n",
      "Time taken: 2.2778480052948 seconds\n",
      "Cosine similarity: 0.8352012634277344\n",
      "Cleaned Cosine similarity: 0.8352012634277344 \n",
      "\n",
      "Chosen: [rp] Illustrate this theory with real-world applications. [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten to create a more concise and informative version of the same content. [/rp]\n",
      "\n",
      "Time taken: 2.4619691371917725 seconds\n",
      "Cosine similarity: 0.4856221675872803\n",
      "Cleaned Cosine similarity: 0.4856221675872803 \n",
      "\n",
      "Chosen: [rp] Shorten this code while maintaining functionality: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten using the prompt \"Optimizing SQL Code for Handling HTTP Requests\"[/rp]\n",
      "\n",
      "Time taken: 2.6184983253479004 seconds\n",
      "Cosine similarity: 0.47830185294151306\n",
      "Cleaned Cosine similarity: 0.47830185294151306 \n",
      "\n",
      "Average Cosine Similarity: 0.47891422510147097\n",
      "\n",
      "\n",
      "*** Next ***\n",
      "Step: 40, rewards/chosen: 0.0015638777986168861, rewards/rejected: -0.013472743332386017, rewards/accuracies: 0.45625001192092896, rewards/margins: 0.015036622062325478, logps/rejected: -84.76504516601562, logps/chosen: -46.32152557373047\n",
      "Step: 45, rewards/chosen: -0.0035339784808456898, rewards/rejected: -0.012540970928966999, rewards/accuracies: 0.375, rewards/margins: 0.009006991982460022, logps/rejected: -73.87751770019531, logps/chosen: -42.682884216308594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen: [rp] Provide a more comprehensive analysis of the themes discussed in the following excerpt, offering deeper insights and explanations: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten to explore themes such as human curiosity and ambition, science and technological advancement, international cooperation and human unity, inspiration, and the future of space exploration. [/rp]\n",
      "\n",
      "Time taken: 3.8920950889587402 seconds\n",
      "Cosine similarity: 0.5923182964324951\n",
      "Cleaned Cosine similarity: 0.5923182964324951 \n",
      "\n",
      "Chosen: [rp] Improve this code by enhancing readability and applying best practices: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was converted using the `methods` library. [/rp]\n",
      "\n",
      "Time taken: 2.04838490486145 seconds\n",
      "Cosine similarity: 0.6267775893211365\n",
      "Cleaned Cosine similarity: 0.6267775893211365 \n",
      "\n",
      "Chosen: [rp] Convert this into a blues song [/rp] <|endoftext|>\n",
      "Output:  [rp] The prompt used to convert the original text to the rewritten text is \"Climate Change\"[/rp]\n",
      "\n",
      "Time taken: 2.3924922943115234 seconds\n",
      "Cosine similarity: 0.5478548407554626\n",
      "Cleaned Cosine similarity: 0.5478548407554626 \n",
      "\n",
      "Chosen: [rp] Translate this from English to Mandarin [/rp] <|endoftext|>\n",
      "Output:  [rp] The prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the\n",
      "Time taken: 12.64274287223816 seconds\n",
      "Cosine similarity: 0.402070552110672\n",
      "Cleaned Cosine similarity: 0.402070552110672 \n",
      "\n",
      "Chosen: [rp] Edit this story to make it more suspenseful. [/rp] <|endoftext|>\n",
      "Output:  [rp] The rewritten prompt used to convert the original text to the rewritten text is:\n",
      "\n",
      "[rp] The rewritten prompt used to convert the original text to the rewritten text is:\n",
      "\n",
      "[rp] The rewritten prompt used to convert the original text to the rewritten text is:\n",
      "\n",
      "[rp] The rewritten prompt used to convert the original text to the rewritten text is:\n",
      "\n",
      "[rp] The rewritten prompt used to convert the original text to the rewritten text is:\n",
      "\n",
      "[rp] The rewritten prompt used to convert the original text to the rewritten text is:\n",
      "\n",
      "[rp] The rewritten prompt used to convert the original text to the rewritten text is:\n",
      "\n",
      "[rp\n",
      "Time taken: 12.76337718963623 seconds\n",
      "Cosine similarity: 0.4678497910499573\n",
      "Cleaned Cosine similarity: 0.4678497910499573 \n",
      "\n",
      "Average Cosine Similarity: 0.5273742139339447\n",
      "\n",
      "\n",
      "*** Next ***\n",
      "Step: 50, rewards/chosen: 0.00611334340646863, rewards/rejected: -0.011429485864937305, rewards/accuracies: 0.48750001192092896, rewards/margins: 0.017542829737067223, logps/rejected: -88.74031829833984, logps/chosen: -44.9675407409668\n",
      "Step: 55, rewards/chosen: 0.005986618809401989, rewards/rejected: -0.01023649238049984, rewards/accuracies: 0.46875, rewards/margins: 0.016223110258579254, logps/rejected: -78.39716339111328, logps/chosen: -44.36798095703125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen: [rp] Bring the setting to life in the following passage by using descriptive language to create a vivid atmosphere: [/rp] <|endoftext|>\n",
      "Output:  [rp] The prompt used to convert the original text to the rewritten text was to use descriptive language to bring the setting to life. [/rp]\n",
      "\n",
      "Time taken: 2.977933883666992 seconds\n",
      "Cosine similarity: 0.7547802925109863\n",
      "Cleaned Cosine similarity: 0.7547802925109863 \n",
      "\n",
      "Chosen: [rp] Shift this guide from a personal account to a general advice format. [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten to provide general advice on the challenges faced by characters in achieving their dreams and aspirations. [/rp]\n",
      "\n",
      "Time taken: 2.7802793979644775 seconds\n",
      "Cosine similarity: 0.5448042750358582\n",
      "Cleaned Cosine similarity: 0.5448042750358582 \n",
      "\n",
      "Chosen: [rp] Illustrate this theory with real-world applications. [/rp] <|endoftext|>\n",
      "Output:  [rp] The prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the rewritten text is [rp] the prompt used to convert the original text to the\n",
      "Time taken: 12.83146047592163 seconds\n",
      "Cosine similarity: 0.25416553020477295\n",
      "Cleaned Cosine similarity: 0.25416553020477295 \n",
      "\n",
      "Chosen: [rp] Immerse yourself in a detailed analysis of the ideas presented in the following text, unraveling its complexities and intricacies to offer a rich tapestry of understanding: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten to create a new text that is more suitable for a high school level audience. [/rp]\n",
      "\n",
      "Time taken: 2.7452807426452637 seconds\n",
      "Cosine similarity: 0.635798454284668\n",
      "Cleaned Cosine similarity: 0.635798454284668 \n",
      "\n",
      "Chosen: [rp] Amplify the emotions expressed in this narrative. [/rp] <|endoftext|>\n",
      "Output:  [rp] The prompt used to convert the original text to the rewritten text was to amplify the emotions in the narrative. [/rp]\n",
      "\n",
      "Time taken: 2.7530126571655273 seconds\n",
      "Cosine similarity: 0.8068079352378845\n",
      "Cleaned Cosine similarity: 0.8068079352378845 \n",
      "\n",
      "Average Cosine Similarity: 0.599271297454834\n",
      "\n",
      "\n",
      "*** Next ***\n",
      "Step: 60, rewards/chosen: 0.0053593069314956665, rewards/rejected: -0.006425981409847736, rewards/accuracies: 0.4375, rewards/margins: 0.011785288341343403, logps/rejected: -78.1056137084961, logps/chosen: -41.115089416503906\n",
      "Step: 65, rewards/chosen: 0.004671192727982998, rewards/rejected: -0.009139266796410084, rewards/accuracies: 0.44999998807907104, rewards/margins: 0.013810458593070507, logps/rejected: -78.21892547607422, logps/chosen: -43.634132385253906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen: [rp] Make this into a limerick [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten to create a limerick about a cognitive scientist named Sue and her research on decision-making. [/rp]\n",
      "\n",
      "Time taken: 2.9008264541625977 seconds\n",
      "Cosine similarity: 0.6180184483528137\n",
      "Cleaned Cosine similarity: 0.6180184483528137 \n",
      "\n",
      "Chosen: [rp] Convert this into a pop song [/rp] <|endoftext|>\n",
      "Output:  [rp] The prompt used to convert the original text to the rewritten text was to create a song with a focus on the moral and legal complexities surrounding the concept of the significance of education. The song should have a positive tone and encourage listeners to invest in education. [/rp]\n",
      "\n",
      "Time taken: 5.2455010414123535 seconds\n",
      "Cosine similarity: 0.4846142530441284\n",
      "Cleaned Cosine similarity: 0.4846142530441284 \n",
      "\n",
      "Chosen: [rp] Draft an elevator pitch based on this research paper. [/rp] <|endoftext|>\n",
      "Output:  [rp] The cultural evolution of language and communication is a vast and complex topic, with roots stretching back to the earliest human prehistory. [rp] Early humans likely communicated using a combination of vocalizations, gestures, and body language. Over time, these rudimentary forms of communication evolved into more complex systems of spoken language, which allowed for the sharing of ideas, the development of culture, and the formation of larger social groups. [rp] The advent of written language some 5,000 years ago represented a major milestone in the cultural evolution of communication. Writing systems allowed for the recording of language and the preservation of knowledge across time and space, enabling the accumulation of cultural knowledge and the development of complex societies. [rp] With\n",
      "Time taken: 12.871435165405273 seconds\n",
      "Cosine similarity: 0.22388309240341187\n",
      "Cleaned Cosine similarity: 0.22388309240341187 \n",
      "\n",
      "Chosen: [rp] Improve this code by enhancing readability and applying best practices: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten to improve readability and adhere to best practices. [/rp]\n",
      "\n",
      "Time taken: 2.2087090015411377 seconds\n",
      "Cosine similarity: 0.6205748915672302\n",
      "Cleaned Cosine similarity: 0.6205748915672302 \n",
      "\n",
      "Chosen: [rp] Amplify the emotions expressed in this narrative. [/rp] <|endoftext|>\n",
      "Output:  [rp] The prompt used to convert the original text to the rewritten text was to amplify the emotions in the narrative. [/rp]\n",
      "\n",
      "Time taken: 2.6836986541748047 seconds\n",
      "Cosine similarity: 0.8068079352378845\n",
      "Cleaned Cosine similarity: 0.8068079352378845 \n",
      "\n",
      "Average Cosine Similarity: 0.5507797241210938\n",
      "\n",
      "\n",
      "*** Next ***\n",
      "Step: 70, rewards/chosen: 0.0026847124099731445, rewards/rejected: -0.015475841239094734, rewards/accuracies: 0.46875, rewards/margins: 0.018160555511713028, logps/rejected: -74.57317352294922, logps/chosen: -41.99445343017578\n",
      "Step: 75, rewards/chosen: 0.006182771176099777, rewards/rejected: -0.014021831564605236, rewards/accuracies: 0.4437499940395355, rewards/margins: 0.02020460180938244, logps/rejected: -71.32490539550781, logps/chosen: -41.83233642578125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen: [rp] Improve this code by enhancing readability and applying best practices: [/rp] <|endoftext|>\n",
      "Output:  [rp] <p>User authentication is an essential aspect of web development that ensures users are who they claim to be. There are various solutions for managing user authentication in HTML, each with its pros and cons. Here are some popular solutions and how they compare:</p>\n",
      "[/rp]\n",
      "\n",
      "Time taken: 5.031920433044434 seconds\n",
      "Cosine similarity: 0.3846920430660248\n",
      "Cleaned Cosine similarity: 0.3846920430660248 \n",
      "\n",
      "Chosen: [rp] Make this into a free verse poem [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten using the prompt \"AI and Ethics\" [rp]\n",
      "\n",
      "Time taken: 1.8781311511993408 seconds\n",
      "Cosine similarity: 0.47709423303604126\n",
      "Cleaned Cosine similarity: 0.47709423303604126 \n",
      "\n",
      "Chosen: [rp] Shorten this code while maintaining functionality: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original code was rewritten to be more concise and readable. [/rp]\n",
      "\n",
      "Time taken: 1.8534963130950928 seconds\n",
      "Cosine similarity: 0.6086036562919617\n",
      "Cleaned Cosine similarity: 0.6086036562919617 \n",
      "\n",
      "Chosen: [rp] Convert this into a blues song [/rp] <|endoftext|>\n",
      "Output:  [rp] The prompt used to convert the original text to the rewritten text was to analyze the differences between the original and the rewritten text in terms of tone, style, and content. [/rp]\n",
      "\n",
      "Time taken: 3.4336562156677246 seconds\n",
      "Cosine similarity: 0.5150067210197449\n",
      "Cleaned Cosine similarity: 0.5150067210197449 \n",
      "\n",
      "Chosen: [rp] Highlight the divergent conclusions drawn by these studies. [/rp] <|endoftext|>\n",
      "Output:  [rp] The text highlights several challenges facing the importance of sports in society. These challenges include the commercialization of sports, the decline in youth participation, the need for greater diversity, equity, and inclusion, and the challenge of maintaining the integrity of sports. However, the text does not draw any divergent conclusions. It primarily focuses on outlining the challenges facing the importance of sports in society and emphasizes the need to address them to ensure sports continue to play a positive role. Therefore, the text does not highlight any divergent conclusions drawn by the studies. [rp]\n",
      "\n",
      "Question: Determine what rewrite prompt was used to convert the original to the rewritten text. Output your answer in between tags like so [rp] your output\n",
      "Time taken: 11.899728298187256 seconds\n",
      "Cosine similarity: 0.27993154525756836\n",
      "Cleaned Cosine similarity: 0.27993154525756836 \n",
      "\n",
      "Average Cosine Similarity: 0.45306563973426817\n",
      "\n",
      "\n",
      "*** Next ***\n",
      "Step: 80, rewards/chosen: 0.008189534768462181, rewards/rejected: -0.007853317074477673, rewards/accuracies: 0.45625001192092896, rewards/margins: 0.01604285277426243, logps/rejected: -72.5776138305664, logps/chosen: -43.13642501831055\n",
      "Step: 85, rewards/chosen: 0.007261964026838541, rewards/rejected: -0.0121129360049963, rewards/accuracies: 0.40625, rewards/margins: 0.019374899566173553, logps/rejected: -79.16297912597656, logps/chosen: -41.64612579345703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen: [rp] Shorten this code while maintaining functionality: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original code was rewritten to be more concise and readable. [/rp]\n",
      "\n",
      "Time taken: 2.129952907562256 seconds\n",
      "Cosine similarity: 0.6086036562919617\n",
      "Cleaned Cosine similarity: 0.6086036562919617 \n",
      "\n",
      "Chosen: [rp] Convert this Python code to Java: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was converted from Python to Java[/rp]\n",
      "\n",
      "Time taken: 1.806962251663208 seconds\n",
      "Cosine similarity: 0.781430184841156\n",
      "Cleaned Cosine similarity: 0.781430184841156 \n",
      "\n",
      "Chosen: [rp] Shift this guide from a personal account to a general advice format. [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten to address the challenges facing the significance of education. The rewritten text highlights the challenges of access, quality, relevance, and discrimination in education systems. It emphasizes the need for collaborative efforts to ensure equitable and inclusive education for all. [/rp]\n",
      "\n",
      "Time taken: 5.541624069213867 seconds\n",
      "Cosine similarity: 0.20178569853305817\n",
      "Cleaned Cosine similarity: 0.20178569853305817 \n",
      "\n",
      "Chosen: [rp] Embark on an extended journey of interpretation and analysis as you delve into the depths of the following excerpt, navigating through its labyrinthine corridors of thought to unveil profound truths and compelling arguments: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten to address common misconceptions about existentialist philosophy and highlight its core themes. [/rp]\n",
      "\n",
      "Time taken: 2.6023268699645996 seconds\n",
      "Cosine similarity: 0.6186161041259766\n",
      "Cleaned Cosine similarity: 0.6186161041259766 \n",
      "\n",
      "Chosen: [rp] Shorten this code while maintaining functionality: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten using the prompt \"Optimizing SQL Code for Handling HTTP Requests\"[/rp]\n",
      "\n",
      "Time taken: 2.6397714614868164 seconds\n",
      "Cosine similarity: 0.47830185294151306\n",
      "Cleaned Cosine similarity: 0.47830185294151306 \n",
      "\n",
      "Average Cosine Similarity: 0.537747499346733\n",
      "\n",
      "\n",
      "*** Next ***\n",
      "Step: 90, rewards/chosen: 0.008250297978520393, rewards/rejected: -0.015863720327615738, rewards/accuracies: 0.512499988079071, rewards/margins: 0.024114016443490982, logps/rejected: -81.17477416992188, logps/chosen: -44.059452056884766\n",
      "Step: 95, rewards/chosen: 0.011418605223298073, rewards/rejected: -0.011813053861260414, rewards/accuracies: 0.543749988079071, rewards/margins: 0.023231659084558487, logps/rejected: -81.0667953491211, logps/chosen: -46.683082580566406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen: [rp] Convert this into a rock song [/rp] <|endoftext|>\n",
      "Output:  [rp] The prompt used to convert the original text to the rewritten text is \"Quantum Mechanics: A Brief Overview\"[/rp]\n",
      "\n",
      "Time taken: 2.6271896362304688 seconds\n",
      "Cosine similarity: 0.4255119562149048\n",
      "Cleaned Cosine similarity: 0.4255119562149048 \n",
      "\n",
      "Chosen: [rp] Convert this into a blues song [/rp] <|endoftext|>\n",
      "Output:  [rp] The prompt used to convert the original text to the rewritten text was to incorporate the concept of intersectionality and expand the scope of social justice to include issues such as economic inequality, racial justice, gender and LGBTQ+ justice, immigration justice, and disability justice. [/rp]\n",
      "\n",
      "Time taken: 4.8695313930511475 seconds\n",
      "Cosine similarity: 0.3477204144001007\n",
      "Cleaned Cosine similarity: 0.3477204144001007 \n",
      "\n",
      "Chosen: [rp] Optimize this code for better performance: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten using the following prompt:\n",
      "\n",
      "```c++\n",
      "#include <iostream>\n",
      "#include <algorithm>\n",
      "#include <vector>\n",
      "\n",
      "using namespace std;\n",
      "\n",
      "bool compare(int a, int b) {\n",
      "  return a - b;\n",
      "}\n",
      "\n",
      "int main() {\n",
      "  vector<int> arr1 = {5, 3, 1, 4, 2};\n",
      "  sort(arr1.begin(), arr1.end());\n",
      "  cout << \"Sorted array in ascending order: \";\n",
      "  for (int i = 0; i < arr1.size(); i++) {\n",
      "    cout << arr1[\n",
      "Time taken: 11.99960732460022 seconds\n",
      "Cosine similarity: 0.37297382950782776\n",
      "Cleaned Cosine similarity: 0.37297382950782776 \n",
      "\n",
      "Chosen: [rp] Improve this code by enhancing readability and applying best practices: [/rp] <|endoftext|>\n",
      "Output:  [rp] The original text was rewritten using a more concise and readable code. [/rp]\n",
      "\n",
      "Time taken: 1.9352915287017822 seconds\n",
      "Cosine similarity: 0.6876615285873413\n",
      "Cleaned Cosine similarity: 0.6876615285873413 \n",
      "\n",
      "Chosen: [rp] Put a humorous spin on the following passage and entertain us with your witty commentary: [/rp] <|endoftext|>\n",
      "Output:  [rp] The prompt used to convert the original text to the rewritten text was to add a humorous and sarcastic tone to the original text, while still conveying the same information. [/rp]\n",
      "\n",
      "Time taken: 3.478186845779419 seconds\n",
      "Cosine similarity: 0.6872568130493164\n",
      "Cleaned Cosine similarity: 0.6872568130493164 \n",
      "\n",
      "Average Cosine Similarity: 0.5042249083518981\n",
      "\n",
      "\n",
      "*** Next ***\n",
      "Step: 100, rewards/chosen: 0.005927668418735266, rewards/rejected: -0.008934211917221546, rewards/accuracies: 0.39375001192092896, rewards/margins: 0.014861879870295525, logps/rejected: -72.31912994384766, logps/chosen: -42.186279296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 103, rewards/chosen: n/a, rewards/rejected: n/a, rewards/accuracies: n/a, rewards/margins: n/a, logps/rejected: n/a, logps/chosen: n/a\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=103, training_loss=0.6868367287719134, metrics={'train_runtime': 4524.4244, 'train_samples_per_second': 0.73, 'train_steps_per_second': 0.023, 'total_flos': 0.0, 'train_loss': 0.6868367287719134, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# from https://github.com/mlabonne/llm-course/blob/main/Fine_tune_a_Mistral_7b_model_with_DPO.ipynb\n",
    "lora_dropout=0.05 #0.5\n",
    "lora_r=32 #\n",
    "lora_alpha=64 #\n",
    "learning_rate=1e-7 # 5e-4 5e-5\n",
    "batch_size = 4\n",
    "dpo_beta = 0.2 # 0.1\n",
    "weight_decay=0.01,  # Weight decay\n",
    "epochs = 1\n",
    "\n",
    "\n",
    "def create_peft_config(model):\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        lora_dropout=lora_dropout,\n",
    "        lora_alpha=lora_alpha,\n",
    "        r=lora_r,\n",
    "        bias=\"none\",\n",
    "        #target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], #decent results \n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"dense\", \"fc1\", \"fc2\", \"lm_head\"] #, \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return model, peft_config\n",
    "\n",
    "model, lora_config = create_peft_config(base_model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    #weight_decay=weight_decay,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=8,\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_steps=5,  #50\n",
    "    logging_steps=5,\n",
    "    log_level='debug',\n",
    "    num_train_epochs=epochs,\n",
    "    save_steps=1000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    \n",
    ")\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model, # model base_model\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset, # test_dataset dataset dataset[\"train\"]\n",
    "    #test_dataset=dataset[\"test\"],\n",
    "    callbacks=[eval_callback,  MetricsLoggingCallback(), PushToGitHubCallback(output_dir=output_dir, commit_message=\"Update model checkpoint\")],  # Add the custom callback\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=lora_config,\n",
    "    beta=dpo_beta,\n",
    "    max_prompt_length=1024, #changed from 1024\n",
    "    max_length=1024, #1536\n",
    "    #loss_type=\"hinge\",\n",
    ")\n",
    "\n",
    "print(\"Starting trainer...\")\n",
    "trainer.train()\n",
    "\n",
    "#trainable params: 23,592,960 || all params: 2,803,276,800 || trainable%: 0.8416207775129448\n",
    "# r = 128 trainable params: 195,624,960 || all params: 2,975,308,800 || trainable%: 6.574946439172969\n",
    "#r32 a64 trainable params: 48,906,240 || all params: 2,828,590,080 || trainable%: 1.728997083946501\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38492c-4bd5-496d-ac60-5f327e93e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_preference_data(test_dataset, tokenizer, model, sentence_model, num_samples=100):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "    new_samples = []\n",
    "    \n",
    "    # Generate random indices for sampling\n",
    "    #random_indices = np.random.choice(range(len(test_dataset)), num_samples, replace=False)\n",
    "\n",
    "    for test_sample in test_dataset:\n",
    "        #test_sample = test_dataset[idx]\n",
    "        \n",
    "        inputs = tokenizer.encode(test_sample[\"prompt\"], add_special_tokens=False, return_tensors=\"pt\")\n",
    "        input_length = inputs.shape[1]\n",
    "\n",
    "        # Generate output\n",
    "        outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n",
    "        new_tokens = outputs[0, input_length:]\n",
    "        generated_text = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "        # Clean the generated text\n",
    "        #generated_text_cleaned = clean_output_text(generated_text)\n",
    "        \n",
    "        # Compute embeddings and similarity\n",
    "        chosen_embedding = sentence_model.encode(test_sample['chosen'], convert_to_tensor=True)\n",
    "        generated_embedding = sentence_model.encode(generated_text, convert_to_tensor=True)\n",
    "        cosine_similarity = util.cos_sim(chosen_embedding, generated_embedding).item()\n",
    "        \n",
    "        # Calculate rejected score\n",
    "        rejected_score = round(cosine_similarity * 5)\n",
    "\n",
    "        # Create a new sample dictionary\n",
    "        new_sample = {\n",
    "            'prompt': test_sample[\"prompt\"],\n",
    "            'chosen': test_sample[\"chosen\"],\n",
    "            'rejected': generated_text,\n",
    "            'chosen_score': test_sample['chosen_score'],  # Assuming this exists in your test_sample\n",
    "            'rejected_score': rejected_score\n",
    "        }\n",
    "        \n",
    "        new_samples.append(new_sample)\n",
    "        \n",
    "    return new_samples\n",
    "\n",
    "# Assuming test_dataset is a HuggingFace Dataset object or a list of dictionaries\n",
    "test_subset = dataset_dict[\"train\"].select(range(400))  # Example subset, adjust as necessary\n",
    "new_preference_data = generate_preference_data(test_subset, tokenizer, model, sentence_model, num_samples=10)\n",
    "new_preference_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1185c-6c42-4c9c-9837-429463ead464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# todo: during training getting these warning:\n",
    "# i guess this is on the base model, need to check. in that case this is fine\n",
    "# UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
    "\n",
    "# seems that this can be ignored:\n",
    "# Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
    "model_name = \"phi\"\n",
    "output_dir = os.path.join(output_dir, f\"final_checkpoint_{model_name}\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "trainer.tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d80fe1f-33d1-4e96-bdf7-137dc0b771fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def push_changes_to_github(output_dir, commit_message=\"Update model\"):\n",
    "    \"\"\"\n",
    "    Pushes changes in output_dir to the existing GitHub repository.\n",
    "    \n",
    "    Parameters:\n",
    "    - output_dir: Path to the directory containing changes to push.\n",
    "    - commit_message: Commit message for the changes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Add all files to Git\n",
    "        subprocess.run([\"git\", \"add\", \".\"], cwd=output_dir, check=True)\n",
    "        \n",
    "        # Commit changes\n",
    "        subprocess.run([\"git\", \"commit\", \"-m\", commit_message], cwd=output_dir, check=True)\n",
    "        \n",
    "        # Push changes\n",
    "        subprocess.run([\"git\", \"push\"], cwd=output_dir, check=True)\n",
    "        \n",
    "        print(\"Changes successfully pushed to GitHub.\")\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "commit_message = \"Update model with new training data\"  # Customize your commit message\n",
    "push_changes_to_github(output_dir, commit_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70be69-31a9-4f75-ac26-54a8b3697edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
