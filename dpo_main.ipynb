{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8164e8a6-8ac9-42dd-bf38-0fb33f8c6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import transformers\n",
    "import optimum\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from trl import DPOTrainer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "#from time \n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94566def-f6eb-44aa-961f-3bd5c2942383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU Memory: 25.425608704 GB, Free Memory: 25.425608704 GB\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "free_memory = total_memory - torch.cuda.memory_allocated(0)\n",
    "print(f\"Total GPU Memory: {total_memory / 1e9} GB, Free Memory: {free_memory / 1e9} GB\")\n",
    "\n",
    "\n",
    "#import flash-attention\n",
    "\n",
    "model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # \"google/gemma-2b-it\" \"microsoft/phi-2\" \"stabilityai/stablelm-zephyr-3b\"\n",
    "access_token = \"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\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fabb3b1d-b1cb-45e9-bc60-1786954d13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    "    token=access_token\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, token=access_token) #microsoft/phi-2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "output_dir = \"/llm_recovery/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f494a73-0c23-4240-83fc-8b3ece395e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500\n",
      "3499\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>chosen</th>\n",
       "      <th>rejected</th>\n",
       "      <th>chosen_score</th>\n",
       "      <th>rejected_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;|system|&gt;\\nDetermine what rewrite prompt was ...</td>\n",
       "      <td>Engage in a thoughtful exploration of the cont...</td>\n",
       "      <td>The rewrite prompt used to convert the origina...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;|system|&gt;\\nDetermine what rewrite prompt was ...</td>\n",
       "      <td>Immerse yourself in a detailed analysis of the...</td>\n",
       "      <td>The rewrite prompt used to convert the text is...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;|system|&gt;\\nDetermine what rewrite prompt was ...</td>\n",
       "      <td>Add a touch of whimsy to the following text by...</td>\n",
       "      <td>The rewrite prompt used to convert the origina...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;|system|&gt;\\nDetermine what rewrite prompt was ...</td>\n",
       "      <td>Capture the essence of the following text in a...</td>\n",
       "      <td>The rewrite prompt used to convert the text is...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;|system|&gt;\\nDetermine what rewrite prompt was ...</td>\n",
       "      <td>Navigate through the maze of contrasting opini...</td>\n",
       "      <td>The rewrite prompt used to convert the text is...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  <|system|>\\nDetermine what rewrite prompt was ...   \n",
       "1  <|system|>\\nDetermine what rewrite prompt was ...   \n",
       "2  <|system|>\\nDetermine what rewrite prompt was ...   \n",
       "3  <|system|>\\nDetermine what rewrite prompt was ...   \n",
       "4  <|system|>\\nDetermine what rewrite prompt was ...   \n",
       "\n",
       "                                              chosen  \\\n",
       "0  Engage in a thoughtful exploration of the cont...   \n",
       "1  Immerse yourself in a detailed analysis of the...   \n",
       "2  Add a touch of whimsy to the following text by...   \n",
       "3  Capture the essence of the following text in a...   \n",
       "4  Navigate through the maze of contrasting opini...   \n",
       "\n",
       "                                            rejected  chosen_score  \\\n",
       "0  The rewrite prompt used to convert the origina...             5   \n",
       "1  The rewrite prompt used to convert the text is...             5   \n",
       "2  The rewrite prompt used to convert the origina...             5   \n",
       "3  The rewrite prompt used to convert the text is...             5   \n",
       "4  The rewrite prompt used to convert the text is...             5   \n",
       "\n",
       "   rejected_score  \n",
       "0               2  \n",
       "1               2  \n",
       "2               3  \n",
       "3               1  \n",
       "4               1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_path = '/llm_recovery/data_generation/dpo_df_tl.csv'   #'/llm_recovery/data_generation/dpo_dataset_v1.json'\n",
    "dataset = pd.read_csv(dataset_path, index_col=0)\n",
    "print(len(dataset))\n",
    "# Drop rows with NaN values\n",
    "dataset = dataset.dropna()\n",
    "print(len(dataset))\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da0e3490-892c-406d-ae9a-fb90da2232d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the end-of-sequence token to each value in the 'chosen' column\n",
    "dataset['chosen'] = dataset['chosen'].apply(lambda x: x.strip() + ' ' + tokenizer.eos_token)\n",
    "dataset['rejected'] = dataset['rejected'].apply(lambda x: x.strip() + ' ' + tokenizer.eos_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a469a80-b692-421b-bb9b-ea0b32433a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected', 'chosen_score', 'rejected_score', '__index_level_0__'],\n",
      "        num_rows: 3324\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['prompt', 'chosen', 'rejected', 'chosen_score', 'rejected_score', '__index_level_0__'],\n",
      "        num_rows: 175\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(dataset)\n",
    "train_test_split = dataset.train_test_split(test_size=0.05)\n",
    "dataset_dict = DatasetDict(train=train_test_split['train'], test=train_test_split['test'])\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f916cb4-6f32-49a5-a9af-a8a32109a2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen: Add a touch of whimsy to the following text by incorporating special characters and creative formatting to enhance its visual appeal: </s>\n",
      "Output: Original_Text: Sociological theories of deviance have a long and complex history, rooted in both the social and philosophical contexts of their time. Deviance, as a concept, refers to any behavior or characteristic that is considered to be outside of the norms or expectations of a particular group or society. Sociologists have sought to understand why certain behaviors are labeled as deviant, and how these labels are applied and maintained within societies. Early sociological theories of deviance were heavily influenced by the work of French sociologist Ã‰mile Durkheim, who argued that deviance is a normal and necessary aspect of any society. According to Durkheim, deviance serves to reinforce social norms and values\n",
      "Time taken: 5.12097692489624 seconds\n",
      "Cosine similarity: 0.08396825939416885\n",
      "Cleaned Cosine similarity: 0.08396825939416885 \n",
      "\n",
      "Chosen: Immerse the reader in the scene described in the following excerpt with evocative and descriptive language: </s>\n",
      "Output: 1. The Science of Language\n",
      "\n",
      "The scientific study of language, also known as linguistics, has played a crucial role in shaping our understanding of language and communication. Linguistics is the scientific study of language and its structure, including the principles governing the sound systems, word formation, sentence structure, and meaning of language. The scientific principles that underlie linguistics include:\n",
      "\n",
      "a. Phonetics: the study of the physical sounds of human speech\n",
      "b. Phonology: the study of the abstract, mental aspects of speech sounds\n",
      "c. Morphology: the study of the structure of words\n",
      "d. Syntax: the study of sentence structure\n",
      "e. Semantics: the study of meaning\n",
      "\n",
      "Time taken: 4.730015993118286 seconds\n",
      "Cosine similarity: 0.21329322457313538\n",
      "Cleaned Cosine similarity: 0.21329322457313538 \n",
      "\n",
      "Chosen: Immerse the reader in the scene described in the following excerpt with evocative and descriptive language: </s>\n",
      "Output: Original_Text: A dialogue discussing climate change and its impacts:\n",
      "\n",
      "Person A: Hey, have you been following the news about climate change lately?\n",
      "\n",
      "Person B: Yeah, I have. It's pretty scary stuff.\n",
      "\n",
      "Person A: I know, right? I was reading an article the other day about how climate change is causing more frequent and severe weather events, like hurricanes and wildfires.\n",
      "\n",
      "Person B: Yes, and it's not just the weather that's being affected. I've heard that many animal and plant species are at risk of extinction due to rising temperatures and changing habitats.\n",
      "\n",
      "Person A: That's true\n",
      "Time taken: 4.689941883087158 seconds\n",
      "Cosine similarity: 0.2373439222574234\n",
      "Cleaned Cosine similarity: 0.2373439222574234 \n",
      "\n",
      "Chosen: Embark on a journey of intellectual discovery as you dissect the themes and motifs embedded within the following passage, offering perceptive analysis and thought-provoking commentary: </s>\n",
      "Output: The rewrite prompt for this passage is: \"What rewrite prompt would help to highlight the complex topic of women's role in history, and how would it contribute to the overall analysis of the passage?\"\n",
      "\n",
      "The rewrite prompt would be: \"What would a rewrite of this passage look like that emphasizes the importance of incorporating diverse perspectives and methodologies in understanding the role of women in history, and how would it contribute to the overall analysis of the passage?\"\n",
      "\n",
      "The rewritten passage would be: \"The role of women in history is a topic that is deeply intertwined with epistemological assumptions and methodological approaches. Epistemology is the study of knowledge and how it is acquired, and it is cru\n",
      "Time taken: 4.675951242446899 seconds\n",
      "Cosine similarity: 0.39535194635391235\n",
      "Cleaned Cosine similarity: 0.39535194635391235 \n",
      "\n",
      "Chosen: Offer a brief overview of the main themes explored in the following passage: </s>\n",
      "Output: The rewrite prompt for this original text is: \"Social justice is a significant and often contested concept in modern society, and there are a variety of perspectives on what it means and how it should be pursued. The liberal individualist perspective emphasizes individual rights, freedoms, and meritocracy, while the social democratic or democratic socialist perspective emphasizes the role of collective action and government intervention in promoting social justice. These perspectives are often contrasted in debates about issues such as healthcare, education, income inequality, and criminal justice reform. It's important to note that these perspectives are not mutually exclusive, and many people and organizations incorporate elements of\n",
      "Time taken: 4.678031921386719 seconds\n",
      "Cosine similarity: 0.4176273047924042\n",
      "Cleaned Cosine similarity: 0.4176273047924042 \n",
      "\n",
      "Average Cosine Similarity: 0.26951693147420885\n",
      "\n",
      "\n",
      "*** Next ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor result in results:\\n    #print(\"Prompt:\", result[\\'prompt\\'])\\n    print(\"Chosen:\", result[\\'chosen\\'])\\n    print(\"Output:\", result[\\'output\\'])\\n    print(\"Time taken:\", result[\\'time_taken\\'], \"seconds\")\\n    print(\"Cosine similarity:\", result[\\'cosine_similarity\\'], \"\\n\")\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load a sentence transformer model for embedding calculation\n",
    "sentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def evaluate_model_with_similarity(test_dataset, tokenizer, model, num_samples=10):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    #model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    def clean_output_text(output_text):\n",
    "        # Define a list of filler phrases to remove\n",
    "        fillers = [\n",
    "            \"Rewrite prompt:\",\n",
    "            \"Sure, the rewrite prompt was used to convert the following text:\",\n",
    "            \"The rewrite prompt used to convert the text is:\"\n",
    "        ]\n",
    "        \n",
    "        # Iterate through each filler and remove it from the output text\n",
    "        for filler in fillers:\n",
    "            output_text = output_text.replace(filler, \"\").strip()\n",
    "        \n",
    "        return output_text\n",
    "        \n",
    "    results = []\n",
    "    random_indices = random.sample(range(len(test_dataset)), num_samples)\n",
    "\n",
    "    for i in random_indices:\n",
    "        test_sample = test_dataset[i] \n",
    "        \n",
    "        # Assuming 'test_sample' contains 'chosen' which we compare with the output\n",
    "        chosen_text = test_sample['chosen']\n",
    "        \"\"\"\n",
    "        prompt = create_custom_prompt(\n",
    "            tokenizer,\n",
    "            test_sample['original_text'],\n",
    "            test_sample['rewritten_text']\n",
    "        )\n",
    "        \"\"\"\n",
    "        inputs = tokenizer.encode(test_sample[\"prompt\"], add_special_tokens=False, return_tensors=\"pt\")\n",
    "        input_length = inputs.shape[1]\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n",
    "        new_tokens = outputs[0, input_length:]\n",
    "        generated_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "        cleaned_gen_text = clean_output_text(generated_text)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "\n",
    "        # Compute embeddings for both chosen and generated text\n",
    "        chosen_embedding = sentence_model.encode(chosen_text, convert_to_tensor=True)\n",
    "        generated_embedding = sentence_model.encode(generated_text, convert_to_tensor=True)\n",
    "        cleaned_embedding = sentence_model.encode(cleaned_gen_text, convert_to_tensor=True)\n",
    "\n",
    "        # Compute Cosine similarity\n",
    "        cosine_similarity = util.cos_sim(chosen_embedding, generated_embedding).item()\n",
    "        cosine_similarity2 = util.cos_sim(chosen_embedding, cleaned_embedding).item()\n",
    "        \n",
    "        \n",
    "        #similarity_scores = cosine_similarity(prompt_embeddings, prompt_1_embeddings)\n",
    "        #similarity_scores = np.diag(similarity_scores)\n",
    "\n",
    "        results.append({\n",
    "            'prompt': test_sample[\"prompt\"],\n",
    "            'chosen': chosen_text,\n",
    "            'output': generated_text,\n",
    "            'time_taken': time_taken,\n",
    "            'cosine_similarity': cosine_similarity,\n",
    "            'cleaned_cos_sim': cosine_similarity2\n",
    "        })\n",
    "\n",
    "    for result in results:\n",
    "        #print(\"Prompt:\", result['prompt'])\n",
    "        print(\"Chosen:\", result['chosen'])\n",
    "        print(\"Output:\", result['output'])\n",
    "        print(\"Time taken:\", result['time_taken'], \"seconds\")\n",
    "        print(\"Cosine similarity:\", result['cosine_similarity'])\n",
    "        print(\"Cleaned Cosine similarity:\", result['cleaned_cos_sim'], \"\\n\")\n",
    "\n",
    "        # Calculate the average cosine similarity\n",
    "    total_similarity = sum(result['cosine_similarity'] for result in results)\n",
    "    average_similarity = total_similarity / len(results)\n",
    "    \n",
    "    # Print the average similarity\n",
    "    print(\"Average Cosine Similarity:\", average_similarity)\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Next ***\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "results = evaluate_model_with_similarity(dataset_dict[\"train\"], tokenizer, base_model, num_samples=5)\n",
    "\n",
    "\"\"\"\n",
    "for result in results:\n",
    "    #print(\"Prompt:\", result['prompt'])\n",
    "    print(\"Chosen:\", result['chosen'])\n",
    "    print(\"Output:\", result['output'])\n",
    "    print(\"Time taken:\", result['time_taken'], \"seconds\")\n",
    "    print(\"Cosine similarity:\", result['cosine_similarity'], \"\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7633fc2-c740-4cf5-9cc8-c594603c1189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['prompt', 'chosen', 'rejected', 'chosen_score', 'rejected_score', '__index_level_0__'],\n",
      "    num_rows: 3324\n",
      "}) Dataset({\n",
      "    features: ['prompt', 'chosen', 'rejected', 'chosen_score', 'rejected_score', '__index_level_0__'],\n",
      "    num_rows: 175\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Assuming dataset[\"train\"] and dataset[\"test\"] are your original datasets\n",
    "train_subset = dataset_dict[\"train\"].shuffle(seed=42) #.select(range(8000))\n",
    "test_subset = dataset_dict[\"test\"].shuffle(seed=42)\n",
    "\n",
    "# You can now use train_subset and test_subset for training and evaluation\n",
    "print(train_subset, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d406de8f-aecf-4c04-81ed-fc1b5453877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "class PushToGitHubCallback(TrainerCallback):\n",
    "    def __init__(self, output_dir, commit_message=\"Update model\"):\n",
    "        self.output_dir = output_dir\n",
    "        self.commit_message = commit_message\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        print(\"Pushing model checkpoint to GitHub...\")\n",
    "        try:\n",
    "            # Ensure we're in the correct directory\n",
    "            os.chdir(self.output_dir)\n",
    "\n",
    "            # Add all files to Git\n",
    "            subprocess.run([\"git\", \"add\", \".\"], check=True)\n",
    "            \n",
    "            # Commit changes\n",
    "            subprocess.run([\"git\", \"commit\", \"-m\", self.commit_message], check=True)\n",
    "            \n",
    "            # Push changes\n",
    "            subprocess.run([\"git\", \"push\"], check=True)\n",
    "            \n",
    "            print(\"Model checkpoint successfully pushed to GitHub.\")\n",
    "            \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to push to GitHub: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54fcfdac-1c61-43cf-8ef9-824f60f5c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import time\n",
    "\n",
    "class EvaluateCallback(TrainerCallback):\n",
    "    def __init__(self, eval_function, eval_dataset, tokenizer, num_samples=10, eval_steps=50):\n",
    "        \"\"\"\n",
    "        eval_function: The evaluation function to use.\n",
    "        eval_dataset: The dataset to use for evaluation.\n",
    "        tokenizer: The tokenizer for encoding.\n",
    "        num_samples: Number of samples to evaluate.\n",
    "        eval_steps: Frequency of evaluation in terms of training steps.\n",
    "        \"\"\"\n",
    "        self.eval_function = eval_function\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_samples = num_samples\n",
    "        self.eval_steps = eval_steps\n",
    "        self.step_count = 0\n",
    "\n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.eval_steps == 0:\n",
    "            print(\"\\nRunning evaluation...\")\n",
    "            self.eval_function(\n",
    "                test_dataset=self.eval_dataset, \n",
    "                tokenizer=self.tokenizer, \n",
    "                model=model, \n",
    "                num_samples=self.num_samples\n",
    "            )\n",
    "\n",
    "# Instantiate the custom callback\n",
    "eval_callback = EvaluateCallback(\n",
    "    eval_function=evaluate_model_with_similarity,\n",
    "    eval_dataset=test_subset,  # Assuming this is a slice of your dataset\n",
    "    tokenizer=tokenizer,\n",
    "    num_samples=5,  # Adjust the number of samples for evaluation\n",
    "    eval_steps=25  # Evaluate every 50 steps, adjust as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3bc8c30-96b1-4f30-bcef-15a3b1e7c300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaFlashAttention2(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dde552-fc87-4de0-93ba-ca95fecb10ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 12,615,680 || all params: 1,112,664,064 || trainable%: 1.1338264987769031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:272: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:332: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51185b94f5b94d518c1be644bb3cc9e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3324 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Using auto half precision backend\n",
      "Currently training with a batch size of: 8\n",
      "***** Running training *****\n",
      "  Num examples = 3,324\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 416\n",
      "  Number of trainable parameters = 12,615,680\n",
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='416' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 11/416 02:32 < 1:54:26, 0.06 it/s, Epoch 0.10/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# from https://github.com/mlabonne/llm-course/blob/main/Fine_tune_a_Mistral_7b_model_with_DPO.ipynb\n",
    "lora_dropout=0.05 #0.5\n",
    "lora_r=16 #\n",
    "lora_alpha=64 #\n",
    "learning_rate=1e-6 # 5e-4 5e-5\n",
    "batch_size = 8\n",
    "dpo_beta = 0.1 # 0.1\n",
    "weight_decay=0.01,  # Weight decay\n",
    "epochs = 4\n",
    "\n",
    "\n",
    "def create_peft_config(model):\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        lora_dropout=lora_dropout,\n",
    "        lora_alpha=lora_alpha,\n",
    "        r=lora_r,\n",
    "        bias=\"none\",\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], #decent results \n",
    "        #target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"dense\", \"fc1\", \"fc2\"] #, \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return model, peft_config\n",
    "\n",
    "model, lora_config = create_peft_config(base_model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    #weight_decay=weight_decay,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_steps=20,  #50\n",
    "    logging_steps=10,\n",
    "    log_level='debug',\n",
    "    num_train_epochs=epochs,\n",
    "    save_steps=1000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    ")\n",
    "\n",
    "#loss_type = hinge\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model, # model base_model\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset, # test_dataset dataset dataset[\"train\"]\n",
    "    #test_dataset=dataset[\"test\"],\n",
    "    callbacks=[eval_callback, PushToGitHubCallback(output_dir=output_dir, commit_message=\"Update model checkpoint\")],  # Add the custom callback\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=lora_config,\n",
    "    beta=dpo_beta,\n",
    "    max_prompt_length=1024, #changed from 1024\n",
    "    max_length=1024, #1536\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Starting trainer...\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38492c-4bd5-496d-ac60-5f327e93e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_preference_data(test_dataset, tokenizer, model, sentence_model, num_samples=100):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "    new_samples = []\n",
    "    \n",
    "    # Generate random indices for sampling\n",
    "    #random_indices = np.random.choice(range(len(test_dataset)), num_samples, replace=False)\n",
    "\n",
    "    for test_sample in test_dataset:\n",
    "        #test_sample = test_dataset[idx]\n",
    "        \n",
    "        inputs = tokenizer.encode(test_sample[\"prompt\"], add_special_tokens=False, return_tensors=\"pt\")\n",
    "        input_length = inputs.shape[1]\n",
    "\n",
    "        # Generate output\n",
    "        outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n",
    "        new_tokens = outputs[0, input_length:]\n",
    "        generated_text = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "        # Clean the generated text\n",
    "        #generated_text_cleaned = clean_output_text(generated_text)\n",
    "        \n",
    "        # Compute embeddings and similarity\n",
    "        chosen_embedding = sentence_model.encode(test_sample['chosen'], convert_to_tensor=True)\n",
    "        generated_embedding = sentence_model.encode(generated_text, convert_to_tensor=True)\n",
    "        cosine_similarity = util.cos_sim(chosen_embedding, generated_embedding).item()\n",
    "        \n",
    "        # Calculate rejected score\n",
    "        rejected_score = round(cosine_similarity * 5)\n",
    "\n",
    "        # Create a new sample dictionary\n",
    "        new_sample = {\n",
    "            'prompt': test_sample[\"prompt\"],\n",
    "            'chosen': test_sample[\"chosen\"],\n",
    "            'rejected': generated_text,\n",
    "            'chosen_score': test_sample['chosen_score'],  # Assuming this exists in your test_sample\n",
    "            'rejected_score': rejected_score\n",
    "        }\n",
    "        \n",
    "        new_samples.append(new_sample)\n",
    "        \n",
    "    return new_samples\n",
    "\n",
    "# Assuming test_dataset is a HuggingFace Dataset object or a list of dictionaries\n",
    "test_subset = dataset_dict[\"train\"].select(range(400))  # Example subset, adjust as necessary\n",
    "new_preference_data = generate_preference_data(test_subset, tokenizer, model, sentence_model, num_samples=10)\n",
    "new_preference_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1185c-6c42-4c9c-9837-429463ead464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# todo: during training getting these warning:\n",
    "# i guess this is on the base model, need to check. in that case this is fine\n",
    "# UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
    "\n",
    "# seems that this can be ignored:\n",
    "# Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
    "model_name = \"tinyllama\"\n",
    "output_dir = os.path.join(output_dir, f\"final_checkpoint_{model_name}\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "trainer.tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d80fe1f-33d1-4e96-bdf7-137dc0b771fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def push_changes_to_github(output_dir, commit_message=\"Update model\"):\n",
    "    \"\"\"\n",
    "    Pushes changes in output_dir to the existing GitHub repository.\n",
    "    \n",
    "    Parameters:\n",
    "    - output_dir: Path to the directory containing changes to push.\n",
    "    - commit_message: Commit message for the changes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Add all files to Git\n",
    "        subprocess.run([\"git\", \"add\", \".\"], cwd=output_dir, check=True)\n",
    "        \n",
    "        # Commit changes\n",
    "        subprocess.run([\"git\", \"commit\", \"-m\", commit_message], cwd=output_dir, check=True)\n",
    "        \n",
    "        # Push changes\n",
    "        subprocess.run([\"git\", \"push\"], cwd=output_dir, check=True)\n",
    "        \n",
    "        print(\"Changes successfully pushed to GitHub.\")\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "commit_message = \"Update model with new training data\"  # Customize your commit message\n",
    "push_changes_to_github(output_dir, commit_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70be69-31a9-4f75-ac26-54a8b3697edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
