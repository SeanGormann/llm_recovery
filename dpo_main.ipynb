{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8164e8a6-8ac9-42dd-bf38-0fb33f8c6bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import bitsandbytes\n",
    "import accelerate\n",
    "import transformers\n",
    "import optimum\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from trl import DPOTrainer\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "#from time \n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94566def-f6eb-44aa-961f-3bd5c2942383",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "total_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "free_memory = total_memory - torch.cuda.memory_allocated(0)\n",
    "print(f\"Total GPU Memory: {total_memory / 1e9} GB, Free Memory: {free_memory / 1e9} GB\")\n",
    "\n",
    "\n",
    "#import flash-attention\n",
    "\n",
    "model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" # \"google/gemma-2b-it\" \"microsoft/phi-2\" \"stabilityai/stablelm-zephyr-3b\"\n",
    "access_token = \"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\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb3b1d-b1cb-45e9-bc60-1786954d13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    "    token=access_token\n",
    ")\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, token=access_token) #microsoft/phi-2\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.truncation_side = \"left\"\n",
    "output_dir = \"/llm_recovery/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f494a73-0c23-4240-83fc-8b3ece395e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '/llm_recovery/data_generation/dpo_df_tl.csv'   #'/llm_recovery/data_generation/dpo_dataset_v1.json'\n",
    "dataset = pd.read_csv(dataset_path, index_col=0)\n",
    "print(len(dataset))\n",
    "# Drop rows with NaN values\n",
    "dataset = dataset.dropna()\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e3490-892c-406d-ae9a-fb90da2232d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the end-of-sequence token to each value in the 'chosen' column\n",
    "dataset['chosen'] = dataset['chosen'].apply(lambda x: x.strip() + ' ' + tokenizer.eos_token)\n",
    "dataset['rejected'] = dataset['rejected'].apply(lambda x: x.strip() + ' ' + tokenizer.eos_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a469a80-b692-421b-bb9b-ea0b32433a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_pandas(dataset)\n",
    "train_test_split = dataset.train_test_split(test_size=0.05)\n",
    "dataset_dict = DatasetDict(train=train_test_split['train'], test=train_test_split['test'])\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af59472-2eb5-4b6e-adee-780a8630e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(dataset_path)\n",
    "dataset = Dataset.from_pandas(dataset)\n",
    "train_test_split = dataset.train_test_split(test_size=0.01)\n",
    "dataset_dict = DatasetDict(train=train_test_split['train'], test=train_test_split['test'])\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f916cb4-6f32-49a5-a9af-a8a32109a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a sentence transformer model for embedding calculation\n",
    "sentence_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def evaluate_model_with_similarity(test_dataset, tokenizer, model, num_samples=10):\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Move model to the appropriate device\n",
    "    #model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    def clean_output_text(output_text):\n",
    "        # Define a list of filler phrases to remove\n",
    "        fillers = [\n",
    "            \"Rewrite prompt:\",\n",
    "            \"Sure, the rewrite prompt was used to convert the following text:\",\n",
    "            \"The rewrite prompt used to convert the text is:\"\n",
    "        ]\n",
    "        \n",
    "        # Iterate through each filler and remove it from the output text\n",
    "        for filler in fillers:\n",
    "            output_text = output_text.replace(filler, \"\").strip()\n",
    "        \n",
    "        return output_text\n",
    "        \n",
    "    results = []\n",
    "    random_indices = random.sample(range(len(test_dataset)), num_samples)\n",
    "\n",
    "    for i in random_indices:\n",
    "        test_sample = test_dataset[i] \n",
    "        \n",
    "        # Assuming 'test_sample' contains 'chosen' which we compare with the output\n",
    "        chosen_text = test_sample['chosen']\n",
    "        \"\"\"\n",
    "        prompt = create_custom_prompt(\n",
    "            tokenizer,\n",
    "            test_sample['original_text'],\n",
    "            test_sample['rewritten_text']\n",
    "        )\n",
    "        \"\"\"\n",
    "        inputs = tokenizer.encode(test_sample[\"prompt\"], add_special_tokens=False, return_tensors=\"pt\")\n",
    "        input_length = inputs.shape[1]\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n",
    "        new_tokens = outputs[0, input_length:]\n",
    "        generated_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "        cleaned_gen_text = clean_output_text(generated_text)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "\n",
    "        # Compute embeddings for both chosen and generated text\n",
    "        chosen_embedding = sentence_model.encode(chosen_text, convert_to_tensor=True)\n",
    "        generated_embedding = sentence_model.encode(generated_text, convert_to_tensor=True)\n",
    "        cleaned_embedding = sentence_model.encode(cleaned_gen_text, convert_to_tensor=True)\n",
    "\n",
    "        # Compute Cosine similarity\n",
    "        cosine_similarity = util.cos_sim(chosen_embedding, generated_embedding).item()\n",
    "        cosine_similarity2 = util.cos_sim(chosen_embedding, cleaned_embedding).item()\n",
    "        \n",
    "        \n",
    "        #similarity_scores = cosine_similarity(prompt_embeddings, prompt_1_embeddings)\n",
    "        #similarity_scores = np.diag(similarity_scores)\n",
    "\n",
    "        results.append({\n",
    "            'prompt': test_sample[\"prompt\"],\n",
    "            'chosen': chosen_text,\n",
    "            'output': generated_text,\n",
    "            'time_taken': time_taken,\n",
    "            'cosine_similarity': cosine_similarity,\n",
    "            'cleaned_cos_sim': cosine_similarity2\n",
    "        })\n",
    "\n",
    "    for result in results:\n",
    "        #print(\"Prompt:\", result['prompt'])\n",
    "        print(\"Chosen:\", result['chosen'])\n",
    "        print(\"Output:\", result['output'])\n",
    "        print(\"Time taken:\", result['time_taken'], \"seconds\")\n",
    "        print(\"Cosine similarity:\", result['cosine_similarity'])\n",
    "        print(\"Cleaned Cosine similarity:\", result['cleaned_cos_sim'], \"\\n\")\n",
    "\n",
    "        # Calculate the average cosine similarity\n",
    "    total_similarity = sum(result['cosine_similarity'] for result in results)\n",
    "    average_similarity = total_similarity / len(results)\n",
    "    \n",
    "    # Print the average similarity\n",
    "    print(\"Average Cosine Similarity:\", average_similarity)\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Next ***\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "results = evaluate_model_with_similarity(dataset_dict[\"train\"], tokenizer, base_model, num_samples=5)\n",
    "\n",
    "\"\"\"\n",
    "for result in results:\n",
    "    #print(\"Prompt:\", result['prompt'])\n",
    "    print(\"Chosen:\", result['chosen'])\n",
    "    print(\"Output:\", result['output'])\n",
    "    print(\"Time taken:\", result['time_taken'], \"seconds\")\n",
    "    print(\"Cosine similarity:\", result['cosine_similarity'], \"\\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7633fc2-c740-4cf5-9cc8-c594603c1189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Assuming dataset[\"train\"] and dataset[\"test\"] are your original datasets\n",
    "train_subset = dataset_dict[\"train\"].shuffle(seed=42) #.select(range(8000))\n",
    "test_subset = dataset_dict[\"test\"].shuffle(seed=42)\n",
    "\n",
    "# You can now use train_subset and test_subset for training and evaluation\n",
    "print(train_subset, test_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d406de8f-aecf-4c04-81ed-fc1b5453877a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "class PushToGitHubCallback(TrainerCallback):\n",
    "    def __init__(self, output_dir, commit_message=\"Update model\"):\n",
    "        self.output_dir = output_dir\n",
    "        self.commit_message = commit_message\n",
    "\n",
    "    def on_save(self, args, state, control, **kwargs):\n",
    "        print(\"Pushing model checkpoint to GitHub...\")\n",
    "        try:\n",
    "            # Ensure we're in the correct directory\n",
    "            os.chdir(self.output_dir)\n",
    "\n",
    "            # Add all files to Git\n",
    "            subprocess.run([\"git\", \"add\", \".\"], check=True)\n",
    "            \n",
    "            # Commit changes\n",
    "            subprocess.run([\"git\", \"commit\", \"-m\", self.commit_message], check=True)\n",
    "            \n",
    "            # Push changes\n",
    "            subprocess.run([\"git\", \"push\"], check=True)\n",
    "            \n",
    "            print(\"Model checkpoint successfully pushed to GitHub.\")\n",
    "            \n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Failed to push to GitHub: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fcfdac-1c61-43cf-8ef9-824f60f5c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "import time\n",
    "\n",
    "class EvaluateCallback(TrainerCallback):\n",
    "    def __init__(self, eval_function, eval_dataset, tokenizer, num_samples=10, eval_steps=50):\n",
    "        \"\"\"\n",
    "        eval_function: The evaluation function to use.\n",
    "        eval_dataset: The dataset to use for evaluation.\n",
    "        tokenizer: The tokenizer for encoding.\n",
    "        num_samples: Number of samples to evaluate.\n",
    "        eval_steps: Frequency of evaluation in terms of training steps.\n",
    "        \"\"\"\n",
    "        self.eval_function = eval_function\n",
    "        self.eval_dataset = eval_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_samples = num_samples\n",
    "        self.eval_steps = eval_steps\n",
    "        self.step_count = 0\n",
    "\n",
    "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
    "        self.step_count += 1\n",
    "        if self.step_count % self.eval_steps == 0:\n",
    "            print(\"\\nRunning evaluation...\")\n",
    "            self.eval_function(\n",
    "                test_dataset=self.eval_dataset, \n",
    "                tokenizer=self.tokenizer, \n",
    "                model=model, \n",
    "                num_samples=self.num_samples\n",
    "            )\n",
    "\n",
    "# Instantiate the custom callback\n",
    "eval_callback = EvaluateCallback(\n",
    "    eval_function=evaluate_model_with_similarity,\n",
    "    eval_dataset=test_subset,  # Assuming this is a slice of your dataset\n",
    "    tokenizer=tokenizer,\n",
    "    num_samples=20,  # Adjust the number of samples for evaluation\n",
    "    eval_steps=100  # Evaluate every 50 steps, adjust as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bc8c30-96b1-4f30-bcef-15a3b1e7c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dde552-fc87-4de0-93ba-ca95fecb10ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from https://github.com/mlabonne/llm-course/blob/main/Fine_tune_a_Mistral_7b_model_with_DPO.ipynb\n",
    "lora_dropout=0.05 #0.5\n",
    "lora_r=8 #\n",
    "lora_alpha=32 #\n",
    "learning_rate=1e-6 # 5e-4 5e-5\n",
    "batch_size = 4\n",
    "dpo_beta = 0.1 # 0.1\n",
    "weight_decay=0.01,  # Weight decay\n",
    "\n",
    "\n",
    "def create_peft_config(model):\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        lora_dropout=lora_dropout,\n",
    "        lora_alpha=lora_alpha,\n",
    "        r=lora_r,\n",
    "        bias=\"none\",\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"], #decent results \n",
    "        #target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"dense\", \"fc1\", \"fc2\"] #, \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    return model, peft_config\n",
    "\n",
    "model, lora_config = create_peft_config(base_model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    learning_rate=learning_rate,\n",
    "    #weight_decay=weight_decay,\n",
    "    fp16=True,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_steps=20,  #50\n",
    "    logging_steps=10,\n",
    "    log_level='debug',\n",
    "    num_train_epochs=1,\n",
    "    save_steps=1000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"paged_adamw_32bit\",\n",
    ")\n",
    "\n",
    "#loss_type = hinge\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model, # model base_model\n",
    "    ref_model=None,\n",
    "    args=training_args,\n",
    "    train_dataset=train_subset, # test_dataset dataset dataset[\"train\"]\n",
    "    #test_dataset=dataset[\"test\"],\n",
    "    callbacks=[eval_callback, PushToGitHubCallback(output_dir=output_dir, commit_message=\"Update model checkpoint\")],  # Add the custom callback\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=lora_config,\n",
    "    beta=dpo_beta,\n",
    "    max_prompt_length=1024, #changed from 1024\n",
    "    max_length=1024, #1536\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Starting trainer...\")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb38492c-4bd5-496d-ac60-5f327e93e655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_preference_data(test_dataset, tokenizer, model, sentence_model, num_samples=100):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "    new_samples = []\n",
    "    \n",
    "    # Generate random indices for sampling\n",
    "    #random_indices = np.random.choice(range(len(test_dataset)), num_samples, replace=False)\n",
    "\n",
    "    for test_sample in test_dataset:\n",
    "        #test_sample = test_dataset[idx]\n",
    "        \n",
    "        inputs = tokenizer.encode(test_sample[\"prompt\"], add_special_tokens=False, return_tensors=\"pt\")\n",
    "        input_length = inputs.shape[1]\n",
    "\n",
    "        # Generate output\n",
    "        outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=150)\n",
    "        new_tokens = outputs[0, input_length:]\n",
    "        generated_text = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "\n",
    "        # Clean the generated text\n",
    "        #generated_text_cleaned = clean_output_text(generated_text)\n",
    "        \n",
    "        # Compute embeddings and similarity\n",
    "        chosen_embedding = sentence_model.encode(test_sample['chosen'], convert_to_tensor=True)\n",
    "        generated_embedding = sentence_model.encode(generated_text, convert_to_tensor=True)\n",
    "        cosine_similarity = util.cos_sim(chosen_embedding, generated_embedding).item()\n",
    "        \n",
    "        # Calculate rejected score\n",
    "        rejected_score = round(cosine_similarity * 5)\n",
    "\n",
    "        # Create a new sample dictionary\n",
    "        new_sample = {\n",
    "            'prompt': test_sample[\"prompt\"],\n",
    "            'chosen': test_sample[\"chosen\"],\n",
    "            'rejected': generated_text,\n",
    "            'chosen_score': test_sample['chosen_score'],  # Assuming this exists in your test_sample\n",
    "            'rejected_score': rejected_score\n",
    "        }\n",
    "        \n",
    "        new_samples.append(new_sample)\n",
    "        \n",
    "    return new_samples\n",
    "\n",
    "# Assuming test_dataset is a HuggingFace Dataset object or a list of dictionaries\n",
    "test_subset = dataset_dict[\"train\"].select(range(400))  # Example subset, adjust as necessary\n",
    "new_preference_data = generate_preference_data(test_subset, tokenizer, model, sentence_model, num_samples=10)\n",
    "new_preference_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1185c-6c42-4c9c-9837-429463ead464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# todo: during training getting these warning:\n",
    "# i guess this is on the base model, need to check. in that case this is fine\n",
    "# UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
    "\n",
    "# seems that this can be ignored:\n",
    "# Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
    "model_name = \"gemma_2b_it\"\n",
    "output_dir = os.path.join(output_dir, f\"final_checkpoint_{model_name}\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "trainer.tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d80fe1f-33d1-4e96-bdf7-137dc0b771fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "def push_changes_to_github(output_dir, commit_message=\"Update model\"):\n",
    "    \"\"\"\n",
    "    Pushes changes in output_dir to the existing GitHub repository.\n",
    "    \n",
    "    Parameters:\n",
    "    - output_dir: Path to the directory containing changes to push.\n",
    "    - commit_message: Commit message for the changes.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Add all files to Git\n",
    "        subprocess.run([\"git\", \"add\", \".\"], cwd=output_dir, check=True)\n",
    "        \n",
    "        # Commit changes\n",
    "        subprocess.run([\"git\", \"commit\", \"-m\", commit_message], cwd=output_dir, check=True)\n",
    "        \n",
    "        # Push changes\n",
    "        subprocess.run([\"git\", \"push\"], cwd=output_dir, check=True)\n",
    "        \n",
    "        print(\"Changes successfully pushed to GitHub.\")\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Example usage\n",
    "commit_message = \"Update model with new training data\"  # Customize your commit message\n",
    "push_changes_to_github(output_dir, commit_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70be69-31a9-4f75-ac26-54a8b3697edc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
